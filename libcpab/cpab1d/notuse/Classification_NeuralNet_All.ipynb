{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/tensorflow_p3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This cell is included to show what libraries are imported and used in the project\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import math\n",
    "import pickle\n",
    "import timeit\n",
    "\n",
    "from glob import glob\n",
    "from scipy.linalg import expm\n",
    "import bisect\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, model_from_json, Model\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten, Lambda, GlobalAveragePooling1D\n",
    "from keras.layers.convolutional import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is included to define various functions used for computations and analysis\n",
    "def error_rate(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    return 1-acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FCN(num_features, num_classes):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv1D(filters=128, kernel_size=8, padding='valid', activation='linear',\n",
    "                     strides=1, input_shape=(num_features,1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv1D(filters=256, kernel_size=5, padding='valid', activation='linear',\n",
    "                     strides=1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv1D(filters=128, kernel_size=3, padding='valid', activation='linear',\n",
    "                     strides=1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(GlobalAveragePooling1D())\n",
    "\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_wang(num_features, num_classes):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(num_features, input_shape=(num_features,)))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(500))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(500))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(500))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet_wang(num_features, num_classes):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Block 1\n",
    "    model.add(Conv1D(filters=64, kernel_size=8, padding='valid', activation='linear',\n",
    "                     strides=1, input_shape=(num_features,1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv1D(filters=64, kernel_size=5, padding='valid', activation='linear',\n",
    "                     strides=1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv1D(filters=64, kernel_size=3, padding='valid', activation='linear',\n",
    "                     strides=1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    # Block 2\n",
    "    model.add(Conv1D(filters=128, kernel_size=8, padding='valid', activation='linear',\n",
    "                     strides=1, input_shape=(num_features,1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv1D(filters=128, kernel_size=5, padding='valid', activation='linear',\n",
    "                     strides=1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv1D(filters=128, kernel_size=3, padding='valid', activation='linear',\n",
    "                     strides=1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    # Block 3\n",
    "    model.add(Conv1D(filters=128, kernel_size=8, padding='valid', activation='linear',\n",
    "                     strides=1, input_shape=(num_features,1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv1D(filters=128, kernel_size=5, padding='valid', activation='linear',\n",
    "                     strides=1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv1D(filters=128, kernel_size=3, padding='valid', activation='linear',\n",
    "                     strides=1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    # Pooling\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "\n",
    "    # Softmax\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeuralNetwork-DatAug (All data sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of all data sets in UCR Archive\n",
    "PATH = 'UCR_TS_Archive_2015/'\n",
    "data_sets = []\n",
    "\n",
    "for folder_PATH in glob(PATH+'*/'):\n",
    "    \n",
    "    ds = folder_PATH.split(\"/\")[-2]\n",
    "    data_sets.append(ds)\n",
    "    \n",
    "data_sets = np.sort(data_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only augmented data sets\n",
    "ds_aug_list = []\n",
    "for folder_PATH in glob('Augmented_data_sets/'+'*'):\n",
    "    ds_aug = folder_PATH.split(\"/\")[-1]\n",
    "    ds = ds_aug.split(\"_\")[:-1]\n",
    "    ds_aug_list.append('_'.join(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_targ = ['HandOutlines']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'FCN-DatAug_performance'\n",
    "continue_run = True\n",
    "continue_ds = True\n",
    "\n",
    "if continue_run:\n",
    "    with open(table_name, 'rb') as f:\n",
    "        perf_table_net = pickle.load(f)\n",
    "    ds_idx = np.where( data_sets == perf_table_net[-1,0] )[0][0] + 2\n",
    "else:\n",
    "    perf_table_net = np.array(['Data set', '1NN-ED', '1NN-DTW',\n",
    "                               'MLP', 'FCN', 'FCN-DatAug', 'Run time FCN-DatAug'])\n",
    "    ds_idx = 0\n",
    "    \n",
    "ds_done = []\n",
    "if continue_ds:\n",
    "    ds_done = perf_table_net[:,0]\n",
    "    ds_idx = 0\n",
    "    #ds_done = np.concatenate((ds_done,np.array(['CinC_ECG_torso','HandOutlines']))) # Out Of Memory errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read previous performance results on UCR Archive\n",
    "UCR_results = {}\n",
    "\n",
    "lines = [line.rstrip('\\n') for line in open('UCR_results.txt')]\n",
    "\n",
    "for line in lines:\n",
    "    ds,nn_ed,nn_dtw,mlp,fcn,resnet,cote,shape_dtw = line.split(\",\")\n",
    "    UCR_results[ds] = ([nn_ed,nn_dtw,mlp,fcn,resnet,cote,shape_dtw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################# HandOutlines #################################\n"
     ]
    }
   ],
   "source": [
    "for ds in ds_targ:\n",
    "    \n",
    "    print('\\n################################# ' + ds + ' #################################')\n",
    "    \n",
    "    if ds in ds_done:\n",
    "        continue\n",
    "    \n",
    "    if ds not in ds_aug_list:\n",
    "        continue\n",
    "    \n",
    "    perf_table_line = np.array([ds, UCR_results[ds][0], UCR_results[ds][1], UCR_results[ds][2], UCR_results[ds][3]])\n",
    "    \n",
    "    \n",
    "    # Test set\n",
    "    with open(PATH + ds + str('/') + ds + '_TEST', 'r') as f:\n",
    "        \n",
    "        test = f.read().splitlines()\n",
    "        data_set_test = np.array([test[0].split(\",\")])\n",
    "        \n",
    "        for line in test[1:]:\n",
    "            data_set_test = np.append(data_set_test, [line.split(\",\")], axis=0)\n",
    "            \n",
    "    # Augmented training set\n",
    "    with open('Augmented_data_sets/' + ds + '_augmented', 'rb') as f:\n",
    "        augmented_data_set = pickle.load(f)\n",
    "    \n",
    "    # Remove NanNs\n",
    "    augmented_data_set = augmented_data_set[~np.isnan(augmented_data_set).any(axis=1)]\n",
    "\n",
    "    print('Length of augmented training set: ' + str(len(augmented_data_set)))\n",
    "    print(augmented_data_set)\n",
    "    \n",
    "\n",
    "    \n",
    "    # --------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "    # Set up training and test set\n",
    "    train_size_aug = len(augmented_data_set)\n",
    "    test_size = len(data_set_test)\n",
    "    ts_length = len(data_set_test[0])-1\n",
    "\n",
    "    X_train_aug = np.zeros((train_size_aug, ts_length))\n",
    "    y_train_aug = np.zeros(train_size_aug)\n",
    "\n",
    "    X_test = np.zeros((test_size, ts_length))\n",
    "    y_test = np.zeros(test_size)\n",
    "\n",
    "    for i in range(ts_length+1):\n",
    "        # Test\n",
    "        for j in range(test_size):\n",
    "            if i == 0:\n",
    "                y_test[j] = int(data_set_test[j][0])\n",
    "            else:\n",
    "                X_test[j][i-1] = float(data_set_test[j][i])\n",
    "        # Train\n",
    "        for j in range(train_size_aug):\n",
    "            if i == 0:\n",
    "                y_train_aug[j] = int(augmented_data_set[j][0])\n",
    "            else:\n",
    "                X_train_aug[j][i-1] = float(augmented_data_set[j][i])\n",
    "\n",
    "    # Make sure the labels are integers\n",
    "    y_test = y_test.astype(int)\n",
    "    y_train_aug = y_train_aug.astype(int)\n",
    "\n",
    "    # Make sure the labels are zero indexed\n",
    "    num_classes = len(np.unique(y_test))\n",
    "\n",
    "    idx = 0\n",
    "    for label in np.unique(y_test):\n",
    "        y_test[np.where( y_test == label )] = idx\n",
    "        idx += 1\n",
    "\n",
    "    # Convert labels to one-hot encoding\n",
    "    y_train_aug_onehot = np.zeros((train_size_aug, num_classes))\n",
    "    y_train_aug_onehot[np.arange(train_size_aug), y_train_aug] = 1\n",
    "\n",
    "\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    \n",
    "    # Reset tensorflow graph\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    # Setup model\n",
    "    model = FCN(ts_length, num_classes)\n",
    "    \n",
    "    # Set up data for Tensorflow model\n",
    "    X_train = np.reshape(X_train_aug,(X_train_aug.shape[0],X_train_aug.shape[1],1))\n",
    "    y_train_onehot = np.reshape(y_train_aug_onehot,(train_size_aug, num_classes))\n",
    "    \n",
    "    num_outer_batches = 10\n",
    "    X_batches = np.split(X_train, num_outer_batches)\n",
    "    y_batches = np.split(y_train_onehot, num_outer_batches)\n",
    "\n",
    "    # Optimizers\n",
    "    sgd = SGD(lr=0.001, decay=1e-6)\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.01)\n",
    "\n",
    "    # Train the model\n",
    "    batch_size = 500\n",
    "    epochs = 100\n",
    "    validation_split = 0.3\n",
    "    \n",
    "    \n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X_train, y_train_onehot,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=validation_split,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)\n",
    "    \n",
    "    #for batch_idx in range(num_outer_batches):\n",
    "    #    print('########################################## batch_idx ##########################################')\n",
    "    #    model.fit(X_batches[batch_idx], y_batches[batch_idx],\n",
    "    #                batch_size=batch_size,\n",
    "    #                epochs=epochs,\n",
    "    #                validation_split=validation_split,\n",
    "    #                shuffle=True,\n",
    "    #                verbose=1)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Evaluate the model with test data\n",
    "    X_test = np.reshape(X_test,(X_test.shape[0],X_test.shape[1],1))\n",
    "    class_probs = model.predict(X_test)\n",
    "    y_pred = np.argmax(class_probs, axis=1)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    err_rate = error_rate(y_test, y_pred)\n",
    "    \n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "    print('\\nTime (with data augmentation): ' + str(elapsed))\n",
    "    print('Classification accuracy: ' + str(acc))\n",
    "    print('Error rate: ' + str(err_rate))\n",
    "    \n",
    "\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    \n",
    "    # Append and save table\n",
    "    perf_table_line = np.concatenate((perf_table_line,np.array([err_rate, elapsed])))\n",
    "    perf_table_net = np.row_stack((perf_table_net,perf_table_line))\n",
    "\n",
    "    with open(table_name, 'wb') as f:\n",
    "        pickle.dump(perf_table_net, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort and save performance table\n",
    "sorted_idx = np.argsort(perf_table_net[:,0])\n",
    "perf_table_net = perf_table_net[sorted_idx]\n",
    "\n",
    "with open(table_name, 'wb') as f:\n",
    "    pickle.dump(perf_table_net, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(table_name, 'rb') as f:\n",
    "    perf_table_net = pickle.load(f)\n",
    "    \n",
    "print(perf_table_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fcn\n",
      "fcn\n",
      "mlp\n",
      "fcn\n",
      "fcn\n",
      "fcn\n",
      "fcn\n",
      "mlp\n",
      "fcn_dataug\n",
      "fcn\n",
      "fcn\n",
      "fcn\n",
      "fcn\n",
      "nn_dtw\n",
      "fcn_dataug\n",
      "fcn\n",
      "fcn\n",
      "mlp\n",
      "fcn\n",
      "fcn\n",
      "fcn\n",
      "fcn_dataug\n",
      "fcn\n",
      "fcn\n",
      "fcn\n",
      "fcn\n",
      "fcn\n",
      "fcn\n",
      "fcn_dataug\n",
      "fcn\n",
      "fcn\n",
      "fcn\n",
      "fcn\n",
      "mlp\n",
      "fcn\n",
      "fcn\n",
      "nn_dtw\n",
      "fcn\n",
      "fcn\n",
      "fcn\n",
      "fcn\n",
      "fcn\n",
      "fcn\n",
      "fcn\n",
      "fcn\n",
      "fcn\n",
      "fcn\n",
      "fcn\n",
      "nn_ed\n",
      "mlp\n",
      "fcn\n",
      "fcn_dataug\n",
      "fcn_dataug\n",
      "fcn_dataug\n",
      "fcn\n",
      "fcn\n",
      "fcn\n",
      "fcn_dataug\n",
      "fcn\n",
      "fcn\n",
      "fcn\n",
      "fcn\n",
      "fcn\n",
      "fcn\n",
      "fcn\n",
      "fcn_dataug\n",
      "fcn\n",
      "fcn\n",
      "fcn\n",
      "fcn\n",
      "nn_dtw\n",
      "mlp\n",
      "fcn\n",
      "nn_dtw\n",
      "fcn\n",
      "fcn\n",
      "nn_dtw\n",
      "mlp\n",
      "fcn\n",
      "fcn\n",
      "fcn\n",
      "mlp\n"
     ]
    }
   ],
   "source": [
    "win = 0\n",
    "lose = 0\n",
    "for line in perf_table_net[1:]:\n",
    "    (ds,nn_ed,nn_dtw,mlp,fcn,fcn_dataug,time) = line\n",
    "    \n",
    "    if ds == 'Data set':\n",
    "        continue\n",
    "    \n",
    "    win = 'nn_ed'\n",
    "    win_val = nn_ed\n",
    "    if float(nn_dtw) <= float(win_val):\n",
    "        win = 'nn_dtw'\n",
    "        win_val = nn_dtw\n",
    "    if float(mlp) <= float(win_val):\n",
    "        win = 'mlp'\n",
    "        win_val = mlp\n",
    "    if float(fcn) <= float(win_val):\n",
    "        win = 'fcn'\n",
    "        win_val = fcn\n",
    "    if float(fcn_dataug) <= float(win_val):\n",
    "        win = 'fcn_dataug'\n",
    "        win_val = fcn_dataug\n",
    "        \n",
    "    print(win)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP DatAug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of all data sets in UCR Archive\n",
    "PATH = 'UCR_TS_Archive_2015/'\n",
    "data_sets = []\n",
    "\n",
    "for folder_PATH in glob(PATH+'*/'):\n",
    "    \n",
    "    ds = folder_PATH.split(\"/\")[-2]\n",
    "    data_sets.append(ds)\n",
    "    \n",
    "data_sets = np.sort(data_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only augmented data sets\n",
    "ds_aug_list = []\n",
    "for folder_PATH in glob('Augmented_data_sets/'+'*'):\n",
    "    ds_aug = folder_PATH.split(\"/\")[-1]\n",
    "    ds = ds_aug.split(\"_\")[:-1]\n",
    "    ds_aug_list.append('_'.join(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'MLP-DatAug_performance'\n",
    "continue_run = True\n",
    "continue_ds = True\n",
    "\n",
    "if continue_run:\n",
    "    with open(table_name, 'rb') as f:\n",
    "        perf_table_net = pickle.load(f)\n",
    "    ds_idx = np.where( data_sets == perf_table_net[-1,0] )[0][0] + 2\n",
    "else:\n",
    "    perf_table_net = np.array(['Data set', '1NN-ED', '1NN-DTW',\n",
    "                               'MLP', 'FCN', 'MLP-DatAug', 'Run time MLP-DatAug'])\n",
    "    ds_idx = 0\n",
    "    \n",
    "ds_done = []\n",
    "if continue_ds:\n",
    "    ds_done = perf_table_net[:,0]\n",
    "    ds_idx = 0\n",
    "    #ds_done = np.concatenate((ds_done,np.array(['CinC_ECG_torso','HandOutlines']))) # Out Of Memory errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_targ = ['HandOutlines']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read previous performance results on UCR Archive\n",
    "UCR_results = {}\n",
    "\n",
    "lines = [line.rstrip('\\n') for line in open('UCR_results.txt')]\n",
    "\n",
    "for line in lines:\n",
    "    ds,nn_ed,nn_dtw,mlp,fcn,resnet,cote,shape_dtw = line.split(\",\")\n",
    "    UCR_results[ds] = ([nn_ed,nn_dtw,mlp,fcn,resnet,cote,shape_dtw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################# HandOutlines #################################\n",
      "Length of augmented training set: 99000\n",
      "[[ 1.         -2.3312     -2.3312     ... -2.3247     -2.3291\n",
      "  -2.3312    ]\n",
      " [ 1.         -2.3374     -2.3374     ... -2.3374     -2.3374\n",
      "  -2.3374    ]\n",
      " [ 0.         -2.3414     -2.3436     ... -2.3414     -2.3414\n",
      "  -2.3414    ]\n",
      " ...\n",
      " [ 0.         -2.2305     -2.23214577 ... -2.2305     -2.2305\n",
      "  -2.2305    ]\n",
      " [ 1.         -2.2761     -2.2761     ... -2.2761     -2.2761\n",
      "  -2.2761    ]\n",
      " [ 1.         -2.1993     -2.1993     ... -2.19747986 -2.1993\n",
      "  -2.1993    ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/tensorflow_p3/lib/python3.6/site-packages/tensorflow/python/client/session.py:1711: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n",
      "Train on 69300 samples, validate on 29700 samples\n",
      "Epoch 1/100\n",
      "69300/69300 [==============================] - 14s 200us/step - loss: 0.9080 - acc: 0.5079 - val_loss: 0.6874 - val_acc: 0.5311\n",
      "Epoch 2/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.6808 - acc: 0.5625 - val_loss: 0.6475 - val_acc: 0.6317\n",
      "Epoch 3/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.6447 - acc: 0.6211 - val_loss: 0.6110 - val_acc: 0.6609\n",
      "Epoch 4/100\n",
      "69300/69300 [==============================] - 14s 199us/step - loss: 0.6169 - acc: 0.6537 - val_loss: 0.5881 - val_acc: 0.6811\n",
      "Epoch 5/100\n",
      "69300/69300 [==============================] - 14s 199us/step - loss: 0.5968 - acc: 0.6725 - val_loss: 0.5725 - val_acc: 0.6887\n",
      "Epoch 6/100\n",
      "69300/69300 [==============================] - 14s 199us/step - loss: 0.5832 - acc: 0.6843 - val_loss: 0.5637 - val_acc: 0.6962\n",
      "Epoch 7/100\n",
      "69300/69300 [==============================] - 14s 199us/step - loss: 0.5718 - acc: 0.6920 - val_loss: 0.5573 - val_acc: 0.7006\n",
      "Epoch 8/100\n",
      "69300/69300 [==============================] - 14s 199us/step - loss: 0.5655 - acc: 0.6974 - val_loss: 0.5493 - val_acc: 0.7105\n",
      "Epoch 9/100\n",
      "69300/69300 [==============================] - 14s 199us/step - loss: 0.5585 - acc: 0.7047 - val_loss: 0.5530 - val_acc: 0.7042\n",
      "Epoch 10/100\n",
      "69300/69300 [==============================] - 14s 199us/step - loss: 0.5510 - acc: 0.7086 - val_loss: 0.5446 - val_acc: 0.7127\n",
      "Epoch 11/100\n",
      "69300/69300 [==============================] - 14s 199us/step - loss: 0.5466 - acc: 0.7137 - val_loss: 0.5375 - val_acc: 0.7217\n",
      "Epoch 12/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.5406 - acc: 0.7169 - val_loss: 0.5350 - val_acc: 0.7218\n",
      "Epoch 13/100\n",
      "69300/69300 [==============================] - 14s 197us/step - loss: 0.5375 - acc: 0.7208 - val_loss: 0.5310 - val_acc: 0.7221\n",
      "Epoch 14/100\n",
      "69300/69300 [==============================] - 14s 197us/step - loss: 0.5334 - acc: 0.7230 - val_loss: 0.5306 - val_acc: 0.7260\n",
      "Epoch 15/100\n",
      "69300/69300 [==============================] - 14s 197us/step - loss: 0.5298 - acc: 0.7270 - val_loss: 0.5291 - val_acc: 0.7254\n",
      "Epoch 16/100\n",
      "69300/69300 [==============================] - 14s 197us/step - loss: 0.5268 - acc: 0.7280 - val_loss: 0.5270 - val_acc: 0.7279\n",
      "Epoch 17/100\n",
      "69300/69300 [==============================] - 14s 197us/step - loss: 0.5213 - acc: 0.7326 - val_loss: 0.5229 - val_acc: 0.7303\n",
      "Epoch 18/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.5202 - acc: 0.7332 - val_loss: 0.5211 - val_acc: 0.7307\n",
      "Epoch 19/100\n",
      "69300/69300 [==============================] - 14s 197us/step - loss: 0.5174 - acc: 0.7366 - val_loss: 0.5200 - val_acc: 0.7291\n",
      "Epoch 20/100\n",
      "69300/69300 [==============================] - 14s 197us/step - loss: 0.5132 - acc: 0.7402 - val_loss: 0.5193 - val_acc: 0.7327\n",
      "Epoch 21/100\n",
      "69300/69300 [==============================] - 14s 197us/step - loss: 0.5107 - acc: 0.7391 - val_loss: 0.5195 - val_acc: 0.7313\n",
      "Epoch 22/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.5072 - acc: 0.7443 - val_loss: 0.5158 - val_acc: 0.7361\n",
      "Epoch 23/100\n",
      "69300/69300 [==============================] - 14s 197us/step - loss: 0.5046 - acc: 0.7453 - val_loss: 0.5131 - val_acc: 0.7362\n",
      "Epoch 24/100\n",
      "69300/69300 [==============================] - 14s 197us/step - loss: 0.5013 - acc: 0.7481 - val_loss: 0.5106 - val_acc: 0.7375\n",
      "Epoch 25/100\n",
      "69300/69300 [==============================] - 14s 197us/step - loss: 0.4982 - acc: 0.7486 - val_loss: 0.5126 - val_acc: 0.7380\n",
      "Epoch 26/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4949 - acc: 0.7519 - val_loss: 0.5100 - val_acc: 0.7391\n",
      "Epoch 27/100\n",
      "69300/69300 [==============================] - 14s 197us/step - loss: 0.4944 - acc: 0.7525 - val_loss: 0.5082 - val_acc: 0.7399\n",
      "Epoch 28/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4880 - acc: 0.7568 - val_loss: 0.5081 - val_acc: 0.7398\n",
      "Epoch 29/100\n",
      "69300/69300 [==============================] - 14s 197us/step - loss: 0.4886 - acc: 0.7584 - val_loss: 0.5081 - val_acc: 0.7417\n",
      "Epoch 30/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4852 - acc: 0.7574 - val_loss: 0.5054 - val_acc: 0.7426\n",
      "Epoch 31/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4837 - acc: 0.7607 - val_loss: 0.5048 - val_acc: 0.7435\n",
      "Epoch 32/100\n",
      "69300/69300 [==============================] - 14s 197us/step - loss: 0.4804 - acc: 0.7623 - val_loss: 0.5051 - val_acc: 0.7423\n",
      "Epoch 33/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4798 - acc: 0.7641 - val_loss: 0.5023 - val_acc: 0.7447\n",
      "Epoch 34/100\n",
      "69300/69300 [==============================] - 14s 197us/step - loss: 0.4768 - acc: 0.7647 - val_loss: 0.5020 - val_acc: 0.7462\n",
      "Epoch 35/100\n",
      "69300/69300 [==============================] - 14s 197us/step - loss: 0.4754 - acc: 0.7667 - val_loss: 0.5028 - val_acc: 0.7437\n",
      "Epoch 36/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4722 - acc: 0.7705 - val_loss: 0.5018 - val_acc: 0.7466\n",
      "Epoch 37/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4687 - acc: 0.7704 - val_loss: 0.5002 - val_acc: 0.7461\n",
      "Epoch 38/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4679 - acc: 0.7701 - val_loss: 0.5003 - val_acc: 0.7487\n",
      "Epoch 39/100\n",
      "69300/69300 [==============================] - 14s 197us/step - loss: 0.4644 - acc: 0.7717 - val_loss: 0.5019 - val_acc: 0.7478\n",
      "Epoch 40/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4622 - acc: 0.7761 - val_loss: 0.4982 - val_acc: 0.7498\n",
      "Epoch 41/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4627 - acc: 0.7731 - val_loss: 0.4971 - val_acc: 0.7497\n",
      "Epoch 42/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4603 - acc: 0.7766 - val_loss: 0.4978 - val_acc: 0.7512\n",
      "Epoch 43/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4587 - acc: 0.7762 - val_loss: 0.4974 - val_acc: 0.7489\n",
      "Epoch 44/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4549 - acc: 0.7802 - val_loss: 0.4965 - val_acc: 0.7502\n",
      "Epoch 45/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4524 - acc: 0.7814 - val_loss: 0.4967 - val_acc: 0.7501\n",
      "Epoch 46/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4516 - acc: 0.7817 - val_loss: 0.4973 - val_acc: 0.7511\n",
      "Epoch 47/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4487 - acc: 0.7851 - val_loss: 0.4966 - val_acc: 0.7524\n",
      "Epoch 48/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4477 - acc: 0.7839 - val_loss: 0.4952 - val_acc: 0.7528\n",
      "Epoch 49/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4464 - acc: 0.7849 - val_loss: 0.4962 - val_acc: 0.7544\n",
      "Epoch 50/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4441 - acc: 0.7870 - val_loss: 0.4959 - val_acc: 0.7537\n",
      "Epoch 51/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4419 - acc: 0.7878 - val_loss: 0.4952 - val_acc: 0.7552\n",
      "Epoch 52/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4418 - acc: 0.7886 - val_loss: 0.4958 - val_acc: 0.7549\n",
      "Epoch 53/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4381 - acc: 0.7905 - val_loss: 0.4961 - val_acc: 0.7551\n",
      "Epoch 54/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4374 - acc: 0.7898 - val_loss: 0.4946 - val_acc: 0.7551\n",
      "Epoch 55/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4349 - acc: 0.7935 - val_loss: 0.4988 - val_acc: 0.7534\n",
      "Epoch 56/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4332 - acc: 0.7930 - val_loss: 0.4949 - val_acc: 0.7557\n",
      "Epoch 57/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4333 - acc: 0.7951 - val_loss: 0.4949 - val_acc: 0.7568\n",
      "Epoch 58/100\n",
      "69300/69300 [==============================] - 14s 197us/step - loss: 0.4309 - acc: 0.7955 - val_loss: 0.4957 - val_acc: 0.7575\n",
      "Epoch 59/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4296 - acc: 0.7957 - val_loss: 0.4956 - val_acc: 0.7570\n",
      "Epoch 60/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4262 - acc: 0.7982 - val_loss: 0.4972 - val_acc: 0.7562\n",
      "Epoch 61/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4289 - acc: 0.7955 - val_loss: 0.4934 - val_acc: 0.7566\n",
      "Epoch 62/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4251 - acc: 0.7981 - val_loss: 0.4953 - val_acc: 0.7569\n",
      "Epoch 63/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4227 - acc: 0.7992 - val_loss: 0.4957 - val_acc: 0.7581\n",
      "Epoch 64/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4221 - acc: 0.8014 - val_loss: 0.4968 - val_acc: 0.7580\n",
      "Epoch 65/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4198 - acc: 0.7996 - val_loss: 0.4952 - val_acc: 0.7593\n",
      "Epoch 66/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4192 - acc: 0.8024 - val_loss: 0.4959 - val_acc: 0.7606\n",
      "Epoch 67/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4173 - acc: 0.8035 - val_loss: 0.4949 - val_acc: 0.7590\n",
      "Epoch 68/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4170 - acc: 0.8041 - val_loss: 0.4947 - val_acc: 0.7587\n",
      "Epoch 69/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4159 - acc: 0.8039 - val_loss: 0.4957 - val_acc: 0.7592\n",
      "Epoch 70/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4150 - acc: 0.8053 - val_loss: 0.4948 - val_acc: 0.7601\n",
      "Epoch 71/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4133 - acc: 0.8064 - val_loss: 0.4961 - val_acc: 0.7586\n",
      "Epoch 72/100\n",
      "69300/69300 [==============================] - 14s 197us/step - loss: 0.4130 - acc: 0.8061 - val_loss: 0.5000 - val_acc: 0.7596\n",
      "Epoch 73/100\n",
      "69300/69300 [==============================] - 14s 197us/step - loss: 0.4101 - acc: 0.8075 - val_loss: 0.4988 - val_acc: 0.7596\n",
      "Epoch 74/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4100 - acc: 0.8076 - val_loss: 0.4952 - val_acc: 0.7610\n",
      "Epoch 75/100\n",
      "69300/69300 [==============================] - 14s 197us/step - loss: 0.4088 - acc: 0.8068 - val_loss: 0.4960 - val_acc: 0.7600\n",
      "Epoch 76/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4069 - acc: 0.8094 - val_loss: 0.4993 - val_acc: 0.7602\n",
      "Epoch 77/100\n",
      "69300/69300 [==============================] - 14s 197us/step - loss: 0.4070 - acc: 0.8104 - val_loss: 0.4962 - val_acc: 0.7606\n",
      "Epoch 78/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4051 - acc: 0.8113 - val_loss: 0.4998 - val_acc: 0.7614\n",
      "Epoch 79/100\n",
      "69300/69300 [==============================] - 14s 197us/step - loss: 0.4059 - acc: 0.8106 - val_loss: 0.4974 - val_acc: 0.7620\n",
      "Epoch 80/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4030 - acc: 0.8109 - val_loss: 0.4976 - val_acc: 0.7618\n",
      "Epoch 81/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4029 - acc: 0.8137 - val_loss: 0.4980 - val_acc: 0.7620\n",
      "Epoch 82/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.4007 - acc: 0.8127 - val_loss: 0.5003 - val_acc: 0.7628\n",
      "Epoch 83/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.3995 - acc: 0.8125 - val_loss: 0.4997 - val_acc: 0.7615\n",
      "Epoch 84/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.3963 - acc: 0.8154 - val_loss: 0.4990 - val_acc: 0.7606\n",
      "Epoch 85/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.3987 - acc: 0.8138 - val_loss: 0.5003 - val_acc: 0.7623\n",
      "Epoch 86/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.3968 - acc: 0.8155 - val_loss: 0.4991 - val_acc: 0.7626\n",
      "Epoch 87/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.3956 - acc: 0.8172 - val_loss: 0.4986 - val_acc: 0.7616\n",
      "Epoch 88/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.3944 - acc: 0.8163 - val_loss: 0.4996 - val_acc: 0.7625\n",
      "Epoch 89/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.3925 - acc: 0.8187 - val_loss: 0.5005 - val_acc: 0.7628\n",
      "Epoch 90/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.3927 - acc: 0.8170 - val_loss: 0.5007 - val_acc: 0.7628\n",
      "Epoch 91/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.3912 - acc: 0.8177 - val_loss: 0.4999 - val_acc: 0.7627\n",
      "Epoch 92/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.3897 - acc: 0.8191 - val_loss: 0.5030 - val_acc: 0.7625\n",
      "Epoch 93/100\n",
      "69300/69300 [==============================] - 14s 197us/step - loss: 0.3892 - acc: 0.8190 - val_loss: 0.5038 - val_acc: 0.7618\n",
      "Epoch 94/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.3896 - acc: 0.8186 - val_loss: 0.5018 - val_acc: 0.7623\n",
      "Epoch 95/100\n",
      "69300/69300 [==============================] - 14s 197us/step - loss: 0.3882 - acc: 0.8205 - val_loss: 0.5031 - val_acc: 0.7617\n",
      "Epoch 96/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.3878 - acc: 0.8193 - val_loss: 0.5001 - val_acc: 0.7615\n",
      "Epoch 97/100\n",
      "69300/69300 [==============================] - 14s 197us/step - loss: 0.3843 - acc: 0.8230 - val_loss: 0.5028 - val_acc: 0.7626\n",
      "Epoch 98/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.3882 - acc: 0.8207 - val_loss: 0.5020 - val_acc: 0.7625\n",
      "Epoch 99/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.3859 - acc: 0.8215 - val_loss: 0.5028 - val_acc: 0.7620\n",
      "Epoch 100/100\n",
      "69300/69300 [==============================] - 14s 198us/step - loss: 0.3838 - acc: 0.8219 - val_loss: 0.5046 - val_acc: 0.7620\n",
      "\n",
      "Time (with data augmentation): 1371.8303010268137\n",
      "Classification accuracy: 0.843\n",
      "Error rate: 0.15700000000000003\n"
     ]
    }
   ],
   "source": [
    "for ds in ds_targ:\n",
    "    \n",
    "    print('\\n################################# ' + ds + ' #################################')\n",
    "    \n",
    "    if ds in ds_done:\n",
    "        continue\n",
    "    \n",
    "    if ds not in ds_aug_list:\n",
    "        continue\n",
    "    \n",
    "    perf_table_line = np.array([ds, UCR_results[ds][0], UCR_results[ds][1], UCR_results[ds][2], UCR_results[ds][3]])\n",
    "    \n",
    "    \n",
    "    # Test set\n",
    "    with open(PATH + ds + str('/') + ds + '_TEST', 'r') as f:\n",
    "        \n",
    "        test = f.read().splitlines()\n",
    "        data_set_test = np.array([test[0].split(\",\")])\n",
    "        \n",
    "        for line in test[1:]:\n",
    "            data_set_test = np.append(data_set_test, [line.split(\",\")], axis=0)\n",
    "            \n",
    "    # Augmented training set\n",
    "    with open('Augmented_data_sets/' + ds + '_augmented', 'rb') as f:\n",
    "        augmented_data_set = pickle.load(f)\n",
    "    \n",
    "    # Remove NanNs\n",
    "    augmented_data_set = augmented_data_set[~np.isnan(augmented_data_set).any(axis=1)]\n",
    "\n",
    "    print('Length of augmented training set: ' + str(len(augmented_data_set)))\n",
    "    print(augmented_data_set)\n",
    "    \n",
    "\n",
    "    \n",
    "    # --------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "    # Set up training and test set\n",
    "    train_size_aug = len(augmented_data_set)\n",
    "    test_size = len(data_set_test)\n",
    "    ts_length = len(data_set_test[0])-1\n",
    "\n",
    "    X_train_aug = np.zeros((train_size_aug, ts_length))\n",
    "    y_train_aug = np.zeros(train_size_aug)\n",
    "\n",
    "    X_test = np.zeros((test_size, ts_length))\n",
    "    y_test = np.zeros(test_size)\n",
    "\n",
    "    for i in range(ts_length+1):\n",
    "        # Test\n",
    "        for j in range(test_size):\n",
    "            if i == 0:\n",
    "                y_test[j] = int(data_set_test[j][0])\n",
    "            else:\n",
    "                X_test[j][i-1] = float(data_set_test[j][i])\n",
    "        # Train\n",
    "        for j in range(train_size_aug):\n",
    "            if i == 0:\n",
    "                y_train_aug[j] = int(augmented_data_set[j][0])\n",
    "            else:\n",
    "                X_train_aug[j][i-1] = float(augmented_data_set[j][i])\n",
    "\n",
    "    # Make sure the labels are integers\n",
    "    y_test = y_test.astype(int)\n",
    "    y_train_aug = y_train_aug.astype(int)\n",
    "\n",
    "    # Make sure the labels are zero indexed\n",
    "    num_classes = len(np.unique(y_test))\n",
    "\n",
    "    idx = 0\n",
    "    for label in np.unique(y_test):\n",
    "        y_test[np.where( y_test == label )] = idx\n",
    "        idx += 1\n",
    "\n",
    "    # Convert labels to one-hot encoding\n",
    "    y_train_aug_onehot = np.zeros((train_size_aug, num_classes))\n",
    "    y_train_aug_onehot[np.arange(train_size_aug), y_train_aug] = 1\n",
    "\n",
    "\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    \n",
    "    # Reset tensorflow graph\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    # Setup model\n",
    "    model = mlp_wang(ts_length, num_classes)\n",
    "    \n",
    "    \n",
    "    # Set up data for Tensorflow model\n",
    "    X_train = np.reshape(X_train_aug,(X_train_aug.shape[0],X_train_aug.shape[1]))\n",
    "    y_train_onehot = np.reshape(y_train_aug_onehot,(train_size_aug, num_classes))\n",
    "\n",
    "    # Optimizers\n",
    "    sgd = SGD(lr=0.001, decay=1e-6)\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.01)\n",
    "\n",
    "    # Train the model\n",
    "    batch_size = 1000\n",
    "    epochs = 100\n",
    "    validation_split = 0.3\n",
    "    \n",
    "    \n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train_onehot,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=validation_split,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)\n",
    "    \n",
    "\n",
    "    # Evaluate the model with test data\n",
    "    X_test = np.reshape(X_test,(X_test.shape[0],X_test.shape[1]))\n",
    "    class_probs = model.predict(X_test)\n",
    "    y_pred = np.argmax(class_probs, axis=1)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    err_rate = error_rate(y_test, y_pred)\n",
    "    \n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "    print('\\nTime (with data augmentation): ' + str(elapsed))\n",
    "    print('Classification accuracy: ' + str(acc))\n",
    "    print('Error rate: ' + str(err_rate))\n",
    "    \n",
    "\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    \n",
    "    # Append and save table\n",
    "    perf_table_line = np.concatenate((perf_table_line,np.array([err_rate, elapsed])))\n",
    "    perf_table_net = np.row_stack((perf_table_net,perf_table_line))\n",
    "\n",
    "    with open(table_name, 'wb') as f:\n",
    "        pickle.dump(perf_table_net, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort and save performance table\n",
    "sorted_idx = np.argsort(perf_table_net[:,0])\n",
    "perf_table_net = perf_table_net[sorted_idx]\n",
    "\n",
    "with open(table_name, 'wb') as f:\n",
    "    pickle.dump(perf_table_net, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['50words' '0.369' '0.310' '0.288' '0.321' '0.9076923076923077'\n",
      "  '5.596147200005362']\n",
      " ['Adiac' '0.389' '0.396' '0.248' '0.143' '0.5038363171355499'\n",
      "  '694.2159890230032']\n",
      " ['ArrowHead' '0.200' '0.297' '0.177' '0.120' '0.1657142857142857'\n",
      "  '492.504771271997']\n",
      " ['Beef' '0.333' '0.367' '0.167' '0.25' '0.30000000000000004'\n",
      "  '1781.552806110005']\n",
      " ['BeetleFly' '0.250' '0.300' '0.150' '0.050' '0.09999999999999998'\n",
      "  '1949.7589633369935']\n",
      " ['BirdChicken' '0.450' '0.250' '0.200' '0.050' '0.09999999999999998'\n",
      "  '1946.7888806190022']\n",
      " ['CBF' '0.148' '0.003' '0.14' '0' '0.01777777777777778'\n",
      "  '501.71618851700623']\n",
      " ['Car' '0.267' '0.267' '0.167' '0.083' '0.19999999999999996'\n",
      "  '2224.4934930699965']\n",
      " ['ChlorineConcentration' '0.35' '0.352' '0.128' '0.157'\n",
      "  '0.4424479166666667' '626.867258519007']\n",
      " ['Coffee' '0.000' '0.000' '0' '0' '0.0' '1077.7104566500057']\n",
      " ['Computers' '0.424' '0.300' '0.460' '0.152' '0.26' '2780.880527966001']\n",
      " ['Cricket_X' '0.423' '0.246' '0.431' '0.185' '0.34615384615384615'\n",
      "  '1119.0732935039996']\n",
      " ['Cricket_Y' '0.433' '0.256' '0.405' '0.208' '0.2897435897435897'\n",
      "  '1137.0555024760106']\n",
      " ['Cricket_Z' '0.413' '0.246' '0.408' '0.187' '0.34102564102564104'\n",
      "  '1139.032841327993']\n",
      " ['Data set' '1NN-ED' '1NN-DTW' 'MLP' 'FCN' 'FCN-DatAug'\n",
      "  'Run time FCN-DatAug']\n",
      " ['DiatomSizeReduction' '0.065' '0.033' '0.036' '0.07'\n",
      "  '0.9052287581699346' '1.2762079590029316']\n",
      " ['DistalPhalanxOutlineAgeGroup' '0.218' '0.208' '0.173' '0.165'\n",
      "  '0.15000000000000002' '304.7960747609977']\n",
      " ['DistalPhalanxOutlineCorrect' '0.248' '0.232' '0.190' '0.188'\n",
      "  '0.2283333333333334' '306.3078954199882']\n",
      " ['DistalPhalanxTW' '0.273' '0.290' '0.253' '0.210' '0.27'\n",
      "  '305.6501304829944']\n",
      " ['ECG200' '0.120' '0.230' '0.080' '0.100' '0.16000000000000003'\n",
      "  '364.3806127489952']\n",
      " ['ECG5000' '0.075' '0.076' '0.065' '0.059' '0.08066666666666666'\n",
      "  '531.34957450199']\n",
      " ['ECGFiveDays' '0.203' '0.232' '0.03' '0.015' '0.132404181184669'\n",
      "  '513.0889808229986']\n",
      " ['Earthquakes' '0.326' '0.258' '0.208' '0.199' '0.29503105590062106'\n",
      "  '1949.7301966309897']\n",
      " ['ElectricDevices' '0.450' '0.399' '0.420' '0.277' '0.26222279859940345'\n",
      "  '395.6408370749996']\n",
      " ['FISH' '0.217' '0.177' '0.126' '0.029' '0.12' '1769.8550811879977']\n",
      " ['FaceAll' '0.286' '0.192' '0.115' '0.071' '0.12485207100591711'\n",
      "  '496.7700241290004']\n",
      " ['FaceFour' '0.216' '0.170' '0.17' '0.068' '0.09090909090909094'\n",
      "  '1329.0305729329993']\n",
      " ['FacesUCR' '0.231' '0.095' '0.185' '0.052' '0.08341463414634143'\n",
      "  '492.30632043399964']\n",
      " ['FordA' '0.341' '0.438' '0.231' '0.094' '0.12135517911691196'\n",
      "  '1900.7688704990069']\n",
      " ['FordB' '0.442' '0.406' '0.371' '0.117' '0.1647414741474147'\n",
      "  '1429.6826590089986']\n",
      " ['Gun_Point' '0.087' '0.093' '0.067' '0' '0.0' '565.5228839979973']\n",
      " ['Ham' '0.400' '0.533' '0.286' '0.238' '0.2857142857142857'\n",
      "  '1644.4676404089987']\n",
      " ['HandOutlines' '0.199' '0.202' '0.193' '0.224' '0.15700000000000003'\n",
      "  '1371.8303010268137']\n",
      " ['Haptics' '0.630' '0.623' '0.539' '0.449' '0.6168831168831168'\n",
      "  '4310.589335559998']\n",
      " ['Herring' '0.484' '0.469' '0.313' '0.297' '0.34375'\n",
      "  '1945.2935786840098']\n",
      " ['InlineSkate' '0.658' '0.616' '0.649' '0.589' '0.8290909090909091'\n",
      "  '8.475191963996622']\n",
      " ['InsectWingbeatSound' '0.438' '0.645' '0.369' '0.598' '0.65'\n",
      "  '957.1622233150119']\n",
      " ['ItalyPowerDemand' '0.045' '0.05' '0.034' '0.03' '0.05442176870748294'\n",
      "  '87.22909873000754']\n",
      " ['LargeKitchenAppliances' '0.507' '0.205' '0.520' '0.104'\n",
      "  '0.10933333333333328' '2769.9412363350057']\n",
      " ['Lighting2' '0.246' '0.131' '0.279' '0.197' '0.21311475409836067'\n",
      "  '2459.3563211719884']\n",
      " ['Lighting7' '0.425' '0.274' '0.356' '0.137' '0.2465753424657534'\n",
      "  '1204.975036443997']\n",
      " ['MALLAT' '0.086' '0.066' '0.064' '0.02' '0.02857142857142858'\n",
      "  '3976.5934260030044']\n",
      " ['Meat' '0.067' '0.067' '0.067' '0.033' '0.19999999999999996'\n",
      "  '1705.5411152239976']\n",
      " ['MedicalImages' '0.316' '0.263' '0.271' '0.208' '0.2868421052631579'\n",
      "  '374.1863921699987']\n",
      " ['MiddlePhalanxOutlineAgeGroup' '0.260' '0.250' '0.265' '0.232'\n",
      "  '0.35750000000000004' '306.26823081100883']\n",
      " ['MiddlePhalanxOutlineCorrect' '0.247' '0.352' '0.240' '0.205'\n",
      "  '0.2583333333333333' '274.5956012520037']\n",
      " ['MiddlePhalanxTW' '0.439' '0.416' '0.391' '0.388' '0.4987468671679198'\n",
      "  '305.8695097049931']\n",
      " ['MoteStrain' '0.121' '0.165' '0.131' '0.05' '0.11102236421725242'\n",
      "  '319.4903386060032']\n",
      " ['NonInvasiveFatalECG_Thorax1' '0.171' '0.209' '0.058' '0.039'\n",
      "  '0.3414758269720102' '2907.6105847309955']\n",
      " ['NonInvasiveFatalECG_Thorax2' '0.120' '0.135' '0.057' '0.045'\n",
      "  '0.1618320610687023' '2906.6576895119942']\n",
      " ['OSULeaf' '0.479' '0.409' '0.43' '0.012' '0.04958677685950408'\n",
      "  '1621.4839139079995']\n",
      " ['OliveOil' '0.133' '0.167' '0.60' '0.167' '0.1333333333333333'\n",
      "  '1605.2282776720094']\n",
      " ['PhalangesOutlinesCorrect' '0.239' '0.272' '0.170' '0.174'\n",
      "  '0.24242424242424243' '312.93136169199715']\n",
      " ['Phoneme' '0.891' '0.772' '0.902' '0.655' '0.8871308016877637'\n",
      "  '9.661713921988849']\n",
      " ['Plane' '0.038' '0.000' '0.019' '0' '0.0' '544.1040451290028']\n",
      " ['ProximalPhalanxOutlineAgeGroup' '0.215' '0.195' '0.176' '0.151'\n",
      "  '0.13170731707317074' '307.9233847810101']\n",
      " ['ProximalPhalanxOutlineCorrect' '0.192' '0.216' '0.113' '0.100'\n",
      "  '0.08591065292096223' '309.0585595140001']\n",
      " ['ProximalPhalanxTW' '0.292' '0.263' '0.203' '0.190'\n",
      "  '0.25749999999999995' '305.92835168800957']\n",
      " ['RefrigerationDevices' '0.605' '0.536' '0.629' '0.467' '0.496'\n",
      "  '2784.976758947989']\n",
      " ['ScreenType' '0.640' '0.603' '0.592' '0.333' '0.3706666666666667'\n",
      "  '1568.7730717199884']\n",
      " ['ShapeletSim' '0.461' '0.350' '0.517' '0.133' '0.061111111111111116'\n",
      "  '1908.829472144018']\n",
      " ['ShapesAll' '0.248' '0.232' '0.225' '0.102' '0.22333333333333338'\n",
      "  '1962.4533057020017']\n",
      " ['SmallKitchenAppliances' '0.659' '0.357' '0.611' '0.197'\n",
      "  '0.23733333333333329' '2773.8584149779927']\n",
      " ['SonyAIBORobotSurface' '0.305' '0.275' '0.273' '0.032'\n",
      "  '0.0748752079866889' '270.4778046440042']\n",
      " ['SonyAIBORobotSurfaceII' '0.141' '0.169' '0.161' '0.038'\n",
      "  '0.04931794333683104' '253.06706775000202']\n",
      " ['StarLightCurves' '0.151' '0.093' '0.043' '0.033' '0.30342399222923755'\n",
      "  '2568.1797584930027']\n",
      " ['Strawberry' '0.062' '0.060' '0.033' '0.031' '0.037520391517128826'\n",
      "  '881.6199408599932']\n",
      " ['SwedishLeaf' '0.211' '0.208' '0.107' '0.034' '0.07999999999999996'\n",
      "  '484.2095267659897']\n",
      " ['Symbols' '0.100' '0.050' '0.147' '0.038' '0.019095477386934623'\n",
      "  '1518.2116302439827']\n",
      " ['ToeSegmentation1' '0.320' '0.228' '0.399' '0.031'\n",
      "  '0.04385964912280704' '1048.1013851090102']\n",
      " ['ToeSegmentation2' '0.192' '0.162' '0.254' '0.085' '0.1461538461538462'\n",
      "  '1298.5754643389955']\n",
      " ['Trace' '0.240' '0.000' '0.18' '0' '0.050000000000000044'\n",
      "  '1035.743469109002']\n",
      " ['TwoLeadECG' '0.253' '0.096' '0.147' '0' '0.0017559262510974394'\n",
      "  '315.8629214950197']\n",
      " ['Two_Patterns' '0.090' '0.000' '0.114' '0.103' '0.74725'\n",
      "  '5.901498086983338']\n",
      " ['UWaveGestureLibraryAll' '0.052' '0.108' '0.046' '0.174'\n",
      "  '0.28308207705192634' '3663.423332361999']\n",
      " ['Wine' '0.389' '0.426' '0.204' '0.111' '0.2777777777777778'\n",
      "  '882.2311205409933']\n",
      " ['WordsSynonyms' '0.382' '0.351' '0.406' '0.42' '0.554858934169279'\n",
      "  '1023.3828999809921']\n",
      " ['Worms' '0.635' '0.536' '0.657' '0.331' '0.48066298342541436'\n",
      "  '3505.793213558005']\n",
      " ['WormsTwoClass' '0.414' '0.337' '0.403' '0.271' '0.28729281767955805'\n",
      "  '3501.9488942669996']\n",
      " ['synthetic_control' '0.120' '0.007' '0.05' '0.01'\n",
      "  '0.026666666666666616' '235.76256823798758']\n",
      " ['uWaveGestureLibrary_X' '0.261' '0.273' '0.232' '0.246'\n",
      "  '0.379676158570631' '1049.3131935870042']\n",
      " ['uWaveGestureLibrary_Y' '0.338' '0.366' '0.297' '0.275'\n",
      "  '0.39028475711892796' '1193.8417804469937']\n",
      " ['uWaveGestureLibrary_Z' '0.35' '0.342' '0.295' '0.271'\n",
      "  '0.3299832495812395' '1161.2998236860003']\n",
      " ['wafer' '0.005' '0.020' '0.004' '0.003' '0.006164828033744296'\n",
      "  '579.1899211370037']\n",
      " ['yoga' '0.170' '0.164' '0.145' '0.155' '0.44099999999999995'\n",
      "  '1607.7894107550092']]\n"
     ]
    }
   ],
   "source": [
    "with open(table_name, 'rb') as f:\n",
    "    perf_table_net = pickle.load(f)\n",
    "    \n",
    "print(perf_table_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCN and MLP (No DatAug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of all data sets in UCR Archive\n",
    "PATH = 'UCR_TS_Archive_2015/'\n",
    "data_sets = []\n",
    "\n",
    "for folder_PATH in glob(PATH+'*/'):\n",
    "    \n",
    "    ds = folder_PATH.split(\"/\")[-2]\n",
    "    data_sets.append(ds)\n",
    "    \n",
    "data_sets = np.sort(data_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only augmented data sets\n",
    "ds_aug_list = [] # '50words'\n",
    "for folder_PATH in glob('Augmented_data_sets/'+'*'):\n",
    "    ds_aug = folder_PATH.split(\"/\")[-1]\n",
    "    ds = ds_aug.split(\"_\")[:-1]\n",
    "    ds_aug_list.append('_'.join(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'MLP-FCN_performance'\n",
    "continue_run = True\n",
    "continue_ds = True\n",
    "\n",
    "if continue_run:\n",
    "    with open(table_name, 'rb') as f:\n",
    "        perf_table_net = pickle.load(f)\n",
    "    ds_idx = np.where( data_sets == perf_table_net[-1,0] )[0][0] + 2\n",
    "else:\n",
    "    perf_table_net = np.array(['Data set', '1NN-ED', '1NN-DTW',\n",
    "                               'FCN', 'Run time FCN', 'MLP', 'Run time MLP'])\n",
    "    ds_idx = 0\n",
    "    \n",
    "if continue_ds:\n",
    "    ds_done = perf_table_net[:,0]\n",
    "    ds_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read previous performance results on UCR Archive\n",
    "UCR_results = {}\n",
    "\n",
    "lines = [line.rstrip('\\n') for line in open('UCR_results.txt')]\n",
    "\n",
    "for line in lines:\n",
    "    ds,nn_ed,nn_dtw,mlp,fcn,resnet,cote = line.split(\",\")\n",
    "    UCR_results[ds] = ([nn_ed,nn_dtw,mlp,fcn,resnet,cote])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################# 50words #################################\n",
      "\n",
      "################################# Adiac #################################\n",
      "\n",
      "################################# ArrowHead #################################\n",
      "\n",
      "################################# Beef #################################\n",
      "\n",
      "################################# BeetleFly #################################\n",
      "\n",
      "################################# BirdChicken #################################\n",
      "\n",
      "################################# CBF #################################\n",
      "\n",
      "################################# Car #################################\n",
      "\n",
      "################################# ChlorineConcentration #################################\n",
      "\n",
      "################################# CinC_ECG_torso #################################\n",
      "\n",
      "################################# Coffee #################################\n",
      "\n",
      "################################# Computers #################################\n",
      "\n",
      "################################# Cricket_X #################################\n",
      "\n",
      "################################# Cricket_Y #################################\n",
      "\n",
      "################################# Cricket_Z #################################\n",
      "\n",
      "################################# DiatomSizeReduction #################################\n",
      "\n",
      "################################# DistalPhalanxOutlineAgeGroup #################################\n",
      "\n",
      "################################# DistalPhalanxOutlineCorrect #################################\n",
      "\n",
      "################################# DistalPhalanxTW #################################\n",
      "\n",
      "################################# ECG200 #################################\n",
      "\n",
      "################################# ECG5000 #################################\n",
      "\n",
      "################################# ECGFiveDays #################################\n",
      "\n",
      "################################# Earthquakes #################################\n",
      "\n",
      "################################# ElectricDevices #################################\n",
      "\n",
      "################################# FISH #################################\n",
      "\n",
      "################################# FaceAll #################################\n",
      "\n",
      "################################# FaceFour #################################\n",
      "\n",
      "################################# FacesUCR #################################\n",
      "\n",
      "################################# FordA #################################\n",
      "\n",
      "################################# FordB #################################\n",
      "\n",
      "################################# Gun_Point #################################\n",
      "\n",
      "################################# Ham #################################\n",
      "\n",
      "################################# HandOutlines #################################\n",
      "\n",
      "################################# Haptics #################################\n",
      "\n",
      "################################# Herring #################################\n",
      "\n",
      "################################# InlineSkate #################################\n",
      "\n",
      "################################# InsectWingbeatSound #################################\n",
      "\n",
      "################################# ItalyPowerDemand #################################\n",
      "\n",
      "################################# LargeKitchenAppliances #################################\n",
      "\n",
      "################################# Lighting2 #################################\n",
      "\n",
      "################################# Lighting7 #################################\n",
      "\n",
      "################################# MALLAT #################################\n",
      "\n",
      "################################# Meat #################################\n",
      "\n",
      "################################# MedicalImages #################################\n",
      "\n",
      "################################# MiddlePhalanxOutlineAgeGroup #################################\n",
      "\n",
      "################################# MiddlePhalanxOutlineCorrect #################################\n",
      "\n",
      "################################# MiddlePhalanxTW #################################\n",
      "\n",
      "################################# MoteStrain #################################\n",
      "\n",
      "################################# NonInvasiveFatalECG_Thorax1 #################################\n",
      "\n",
      "################################# NonInvasiveFatalECG_Thorax2 #################################\n",
      "\n",
      "################################# OSULeaf #################################\n",
      "\n",
      "################################# OliveOil #################################\n",
      "\n",
      "################################# PhalangesOutlinesCorrect #################################\n",
      "\n",
      "################################# Phoneme #################################\n",
      "\n",
      "################################# Plane #################################\n",
      "\n",
      "################################# ProximalPhalanxOutlineAgeGroup #################################\n",
      "\n",
      "################################# ProximalPhalanxOutlineCorrect #################################\n",
      "\n",
      "################################# ProximalPhalanxTW #################################\n",
      "\n",
      "################################# RefrigerationDevices #################################\n",
      "\n",
      "################################# ScreenType #################################\n",
      "\n",
      "################################# ShapeletSim #################################\n",
      "\n",
      "################################# ShapesAll #################################\n",
      "\n",
      "################################# SmallKitchenAppliances #################################\n",
      "\n",
      "################################# SonyAIBORobotSurface #################################\n",
      "\n",
      "################################# SonyAIBORobotSurfaceII #################################\n",
      "\n",
      "################################# StarLightCurves #################################\n",
      "\n",
      "################################# Strawberry #################################\n",
      "\n",
      "################################# SwedishLeaf #################################\n",
      "\n",
      "################################# Symbols #################################\n",
      "\n",
      "################################# ToeSegmentation1 #################################\n",
      "\n",
      "################################# ToeSegmentation2 #################################\n",
      "\n",
      "################################# Trace #################################\n",
      "\n",
      "################################# TwoLeadECG #################################\n",
      "\n",
      "################################# Two_Patterns #################################\n",
      "\n",
      "################################# UWaveGestureLibraryAll #################################\n",
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n",
      "\n",
      "Time (FCN): 814.5173629475757\n",
      "Classification accuracy: 0.6887213847012842\n",
      "Error rate: 0.3112786152987158\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/tensorflow_p3/lib/python3.6/site-packages/tensorflow/python/client/session.py:1711: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time (MLP): 104.67743703350425\n",
      "Classification accuracy: 0.9438860971524288\n",
      "Error rate: 0.05611390284757123\n",
      "\n",
      "################################# Wine #################################\n",
      "\n",
      "################################# WordsSynonyms #################################\n",
      "\n",
      "################################# Worms #################################\n",
      "\n",
      "Time (FCN): 68.61305598262697\n",
      "Classification accuracy: 0.5027624309392266\n",
      "Error rate: 0.49723756906077343\n",
      "\n",
      "\n",
      "Time (MLP): 10.164013951085508\n",
      "Classification accuracy: 0.3701657458563536\n",
      "Error rate: 0.6298342541436464\n",
      "\n",
      "################################# WormsTwoClass #################################\n",
      "\n",
      "Time (FCN): 68.5181339615956\n",
      "Classification accuracy: 0.5303867403314917\n",
      "Error rate: 0.4696132596685083\n",
      "\n",
      "\n",
      "Time (MLP): 10.191812179982662\n",
      "Classification accuracy: 0.5082872928176796\n",
      "Error rate: 0.4917127071823204\n",
      "\n",
      "################################# synthetic_control #################################\n",
      "\n",
      "################################# uWaveGestureLibrary_X #################################\n",
      "\n",
      "Time (FCN): 292.1977254720405\n",
      "Classification accuracy: 0.7194304857621441\n",
      "Error rate: 0.2805695142378559\n",
      "\n",
      "\n",
      "Time (MLP): 49.37008660286665\n",
      "Classification accuracy: 0.7498604131769961\n",
      "Error rate: 0.25013958682300386\n",
      "\n",
      "################################# uWaveGestureLibrary_Y #################################\n",
      "\n",
      "Time (FCN): 292.30519205052406\n",
      "Classification accuracy: 0.6180904522613065\n",
      "Error rate: 0.38190954773869346\n",
      "\n",
      "\n",
      "Time (MLP): 49.401677154935896\n",
      "Classification accuracy: 0.6825795644891123\n",
      "Error rate: 0.3174204355108877\n",
      "\n",
      "################################# uWaveGestureLibrary_Z #################################\n",
      "\n",
      "Time (FCN): 292.2772204261273\n",
      "Classification accuracy: 0.6775544388609716\n",
      "Error rate: 0.32244556113902845\n",
      "\n",
      "\n",
      "Time (MLP): 49.603060017339885\n",
      "Classification accuracy: 0.7029592406476829\n",
      "Error rate: 0.2970407593523171\n",
      "\n",
      "################################# wafer #################################\n",
      "\n",
      "Time (FCN): 185.03571289312094\n",
      "Classification accuracy: 0.9951330304996755\n",
      "Error rate: 0.004866969500324503\n",
      "\n",
      "\n",
      "Time (MLP): 47.95057901274413\n",
      "Classification accuracy: 0.9928617780661908\n",
      "Error rate: 0.007138221933809197\n",
      "\n",
      "################################# yoga #################################\n"
     ]
    }
   ],
   "source": [
    "for ds in data_sets[ds_idx:]:\n",
    "    \n",
    "    print('\\n################################# ' + ds + ' #################################')\n",
    "    \n",
    "    if ds in ds_done:\n",
    "        continue\n",
    "    \n",
    "    if ds not in ds_aug_list:\n",
    "        continue\n",
    "    \n",
    "    perf_table_line = np.array([ds, UCR_results[ds][0], UCR_results[ds][1]])\n",
    "    \n",
    "    # Training set\n",
    "    with open(PATH + ds + str('/') + ds + '_TRAIN', 'r') as f:\n",
    "        \n",
    "        train = f.read().splitlines()\n",
    "        data_set_train = np.array([train[0].split(\",\")])\n",
    "        \n",
    "        for line in train[1:]:\n",
    "            data_set_train = np.append(data_set_train, [line.split(\",\")], axis=0)\n",
    "    \n",
    "    # Test set\n",
    "    with open(PATH + ds + str('/') + ds + '_TEST', 'r') as f:\n",
    "        \n",
    "        test = f.read().splitlines()\n",
    "        data_set_test = np.array([test[0].split(\",\")])\n",
    "        \n",
    "        for line in test[1:]:\n",
    "            data_set_test = np.append(data_set_test, [line.split(\",\")], axis=0)\n",
    "    \n",
    "    # --------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "    # Set up training and test set\n",
    "    train_size = len(data_set_train)\n",
    "    test_size = len(data_set_test)\n",
    "    ts_length = len(data_set_test[0])-1\n",
    "\n",
    "    X_train = np.zeros((train_size, ts_length))\n",
    "    y_train = np.zeros(train_size)\n",
    "\n",
    "    X_test = np.zeros((test_size, ts_length))\n",
    "    y_test = np.zeros(test_size)\n",
    "\n",
    "    for i in range(ts_length+1):\n",
    "        # Test\n",
    "        for j in range(test_size):\n",
    "            if i == 0:\n",
    "                y_test[j] = int(data_set_test[j][0])\n",
    "            else:\n",
    "                X_test[j][i-1] = float(data_set_test[j][i])\n",
    "        # Train\n",
    "        for j in range(train_size):\n",
    "            if i == 0:\n",
    "                y_train[j] = int(data_set_train[j][0])\n",
    "            else:\n",
    "                X_train[j][i-1] = float(data_set_train[j][i])\n",
    "\n",
    "    # Make sure the labels are integers\n",
    "    y_test = y_test.astype(int)\n",
    "    y_train = y_train.astype(int)\n",
    "\n",
    "    # Make sure the labels are zero indexed\n",
    "    num_classes = len(np.unique(y_test))\n",
    "\n",
    "    idx = 0\n",
    "    for label in np.unique(y_test):\n",
    "        y_train[np.where( y_train == label )] = idx\n",
    "        y_test[np.where( y_test == label )] = idx\n",
    "        idx += 1\n",
    "\n",
    "    # Convert labels to one-hot encoding\n",
    "    y_train_onehot = np.zeros((train_size, num_classes))\n",
    "    y_train_onehot[np.arange(train_size), y_train] = 1\n",
    "\n",
    "\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    # ---------------------------------- FCN ----------------------------------\n",
    "    # Reset tensorflow graph\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    # Setup model\n",
    "    model = FCN(ts_length, num_classes)\n",
    "    \n",
    "    # Set up data for Tensorflow model\n",
    "    X_train = np.reshape(X_train,(X_train.shape[0],X_train.shape[1],1))\n",
    "    y_train_onehot = np.reshape(y_train_onehot,(train_size, num_classes))\n",
    "\n",
    "    # Optimizers\n",
    "    sgd = SGD(lr=0.001, decay=1e-6)\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.01)\n",
    "\n",
    "    # Train the model\n",
    "    batch_size = 10\n",
    "    epochs = 100\n",
    "    validation_split = 0.3\n",
    "    \n",
    "    \n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train_onehot,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=validation_split,\n",
    "                    shuffle=True,\n",
    "                    verbose=0)\n",
    "    \n",
    "\n",
    "    # Evaluate the model with test data\n",
    "    X_test = np.reshape(X_test,(X_test.shape[0],X_test.shape[1],1))\n",
    "    class_probs = model.predict(X_test)\n",
    "    y_pred = np.argmax(class_probs, axis=1)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    err_rate = error_rate(y_test, y_pred)\n",
    "    \n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "    print('\\nTime (FCN): ' + str(elapsed))\n",
    "    print('Classification accuracy: ' + str(acc))\n",
    "    print('Error rate: ' + str(err_rate) + '\\n')\n",
    "    \n",
    "    # Append and save table\n",
    "    perf_table_line = np.concatenate((perf_table_line,np.array([err_rate, elapsed])))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # ---------------------------------- MLP ----------------------------------\n",
    "    # Reset tensorflow graph\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    # Setup model\n",
    "    model = mlp_wang(ts_length, num_classes)\n",
    "    \n",
    "    \n",
    "    # Set up data for Tensorflow model\n",
    "    X_train = np.reshape(X_train,(X_train.shape[0],X_train.shape[1]))\n",
    "    y_train_onehot = np.reshape(y_train_onehot,(train_size, num_classes))\n",
    "\n",
    "    # Optimizers\n",
    "    sgd = SGD(lr=0.001, decay=1e-6)\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.01)\n",
    "\n",
    "    # Train the model\n",
    "    batch_size = 10\n",
    "    epochs = 100\n",
    "    validation_split = 0.3\n",
    "    \n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train_onehot,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=validation_split,\n",
    "                    shuffle=True,\n",
    "                    verbose=0)\n",
    "    \n",
    "\n",
    "    # Evaluate the model with test data\n",
    "    X_test = np.reshape(X_test,(X_test.shape[0],X_test.shape[1]))\n",
    "    class_probs = model.predict(X_test)\n",
    "    y_pred = np.argmax(class_probs, axis=1)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    err_rate = error_rate(y_test, y_pred)\n",
    "    \n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "    print('\\nTime (MLP): ' + str(elapsed))\n",
    "    print('Classification accuracy: ' + str(acc))\n",
    "    print('Error rate: ' + str(err_rate))\n",
    "    \n",
    "    \n",
    "    # Append and save table\n",
    "    perf_table_line = np.concatenate((perf_table_line,np.array([err_rate, elapsed])))\n",
    "    perf_table_net = np.row_stack((perf_table_net,perf_table_line))\n",
    "\n",
    "    with open(table_name, 'wb') as f:\n",
    "        pickle.dump(perf_table_net, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort and save performance table\n",
    "sorted_idx = np.argsort(perf_table_net[:,0])\n",
    "perf_table_net = perf_table_net[sorted_idx]\n",
    "\n",
    "with open(table_name, 'wb') as f:\n",
    "    pickle.dump(perf_table_net, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['50words' '0.369' '0.310' '0.621978021978022' '16.156557979993522'\n",
      "  '0.3582417582417582' '8.349115027987864']\n",
      " ['Adiac' '0.389' '0.396' '0.5063938618925832' '80.85867945943028'\n",
      "  '0.6163682864450128' '20.206664296798408']\n",
      " ['ArrowHead' '0.200' '0.297' '0.6971428571428571' '2.0730466549866833'\n",
      "  '0.23428571428571432' '1.1012845349905547']\n",
      " ['Beef' '0.333' '0.367' '0.8' '2.160927275021095' '0.5333333333333333'\n",
      "  '1.0363243660249282']\n",
      " ['BeetleFly' '0.250' '0.300' '0.5' '1.6740828149777371' '0.35'\n",
      "  '0.823900727002183']\n",
      " ['BirdChicken' '0.450' '0.250' '0.5' '1.6738065860117786' '0.4'\n",
      "  '0.8248766429896932']\n",
      " ['CBF' '0.148' '0.003' '0.012222222222222245' '6.720642405562103'\n",
      "  '0.1333333333333333' '2.505319164134562']\n",
      " ['Car' '0.267' '0.267' '0.7666666666666666' '3.5472062949847896'\n",
      "  '0.16666666666666663' '1.5816666040045675']\n",
      " ['ChlorineConcentration' '0.35' '0.352' '0.4854166666666667'\n",
      "  '15.830903065012535' '0.36197916666666663' '8.672713913023472']\n",
      " ['CinC_ECG_torso' '0.103' '0.349' '0.6289855072463768'\n",
      "  '70.13419813103974' '0.27826086956521734' '10.717572479508817']\n",
      " ['Coffee' '0.000' '0.000' '0.4642857142857143' '1.5922197710024193'\n",
      "  '0.0' '0.8207675750018097']\n",
      " ['Computers' '0.424' '0.300' '0.244' '12.878529620007612' '0.508'\n",
      "  '5.173824152996531']\n",
      " ['Cricket_X' '0.423' '0.246' '0.3076923076923077' '14.58842258900404'\n",
      "  '0.48461538461538467' '7.381774018023862']\n",
      " ['Cricket_Y' '0.433' '0.256' '0.3435897435897436' '14.616224493016489'\n",
      "  '0.45641025641025645' '7.376059621019522']\n",
      " ['Cricket_Z' '0.413' '0.246' '0.27179487179487183' '14.622515135008143'\n",
      "  '0.4871794871794872' '8.46353576797992']\n",
      " ['Data set' '1NN-ED' '1NN-DTW' 'FCN' 'Run time FCN' 'MLP' 'Run time MLP']\n",
      " ['DiatomSizeReduction' '0.065' '0.033' '0.892156862745098'\n",
      "  '1.5976789080013987' '0.14379084967320266' '0.8316655980015639']\n",
      " ['DistalPhalanxOutlineAgeGroup' '0.218' '0.208' '0.7075'\n",
      "  '17.809334841556847' '0.72' '7.186572895385325']\n",
      " ['DistalPhalanxOutlineCorrect' '0.248' '0.232' '0.49'\n",
      "  '9.623458820977248' '0.4983333333333333' '5.424003053019987']\n",
      " ['DistalPhalanxTW' '0.273' '0.290' '0.22250000000000003'\n",
      "  '17.614669861271977' '0.21750000000000003' '7.381341560743749']\n",
      " ['ECG200' '0.120' '0.230' '0.17000000000000004' '3.7581706199853215'\n",
      "  '0.10999999999999999' '2.0904087779927067']\n",
      " ['ECG5000' '0.075' '0.076' '0.07199999999999995' '16.665197364985943'\n",
      "  '0.08066666666666666' '9.296949259995017']\n",
      " ['ECGFiveDays' '0.203' '0.232' '0.49709639953542395'\n",
      "  '1.5480156099947635' '0.2137049941927991' '0.839877243997762']\n",
      " ['Earthquakes' '0.326' '0.258' '0.20186335403726707' '6.613371093000751'\n",
      "  '0.1863354037267081' '2.9265396560076624']\n",
      " ['ElectricDevices' '0.450' '0.399' '0.34794449487744783'\n",
      "  '1171.6854627802968' '0.46141875243159125' '415.00985121540725']\n",
      " ['FISH' '0.217' '0.177' '0.19428571428571428' '8.05623600500985'\n",
      "  '0.17142857142857137' '3.6991011240170337']\n",
      " ['FaceAll' '0.286' '0.192' '0.2751479289940828' '18.822522865986684'\n",
      "  '0.3591715976331361' '10.581182547990466']\n",
      " ['FaceFour' '0.216' '0.170' '0.7045454545454546' '1.6387192500114907'\n",
      "  '0.36363636363636365' '0.834445771004539']\n",
      " ['FacesUCR' '0.231' '0.095' '0.1326829268292683' '7.092158535000635'\n",
      "  '0.2775609756097561' '3.947803435003152']\n",
      " ['FordA' '0.341' '0.438' '0.0885865037489586' '650.6696123844013'\n",
      "  '0.2696473201888364' '85.02564455661923']\n",
      " ['FordB' '0.442' '0.406' '0.08580858085808585' '401.948300620541'\n",
      "  '0.4397689768976898' '52.740194101817906']\n",
      " ['Gun_Point' '0.087' '0.093' '0.4933333333333333' '2.483249723998597'\n",
      "  '0.07333333333333336' '1.3667876230028924']\n",
      " ['Ham' '0.400' '0.533' '0.419047619047619' '5.1645477330021095'\n",
      "  '0.24761904761904763' '2.405201824993128']\n",
      " ['HandOutlines' '0.199' '0.202' '0.32399999999999995'\n",
      "  '745.0568043123931' '0.362' '186.3911478444934']\n",
      " ['Haptics' '0.630' '0.623' '0.5551948051948052' '10.753740154003026'\n",
      "  '0.5746753246753247' '3.7943686670041643']\n",
      " ['Herring' '0.484' '0.469' '0.59375' '3.495426173991291' '0.34375'\n",
      "  '1.6091016159916762']\n",
      " ['InlineSkate' '0.658' '0.616' '0.8090909090909091' '10.262237232003827'\n",
      "  '0.7036363636363636' '3.343072410003515']\n",
      " ['InsectWingbeatSound' '0.438' '0.645' '0.6575757575757576'\n",
      "  '8.712094968999736' '0.4181818181818182' '4.517587498005014']\n",
      " ['ItalyPowerDemand' '0.045' '0.05' '0.07677356656948497'\n",
      "  '3.03491672599921' '0.04178814382896012' '1.6795974810083862']\n",
      " ['LargeKitchenAppliances' '0.507' '0.205' '0.2666666666666667'\n",
      "  '244.01764256041497' '0.5920000000000001' '33.58745793718845']\n",
      " ['Lighting2' '0.246' '0.131' '0.3770491803278688' '3.7721701490227133'\n",
      "  '0.29508196721311475' '1.6518157289829105']\n",
      " ['Lighting7' '0.425' '0.274' '0.3972602739726028' '3.301852261007298'\n",
      "  '0.4246575342465754' '1.6777177500189282']\n",
      " ['MALLAT' '0.086' '0.066' '0.8754797441364606' '4.528807807015255'\n",
      "  '0.06481876332622605' '1.62976510199951']\n",
      " ['Meat' '0.067' '0.067' '0.6666666666666667' '3.412667412980227'\n",
      "  '0.3666666666666667' '1.6269853909907397']\n",
      " ['MedicalImages' '0.316' '0.263' '0.2723684210526316'\n",
      "  '14.533391961012967' '0.29210526315789476' '7.439912298985291']\n",
      " ['MiddlePhalanxOutlineAgeGroup' '0.260' '0.250' '0.7725'\n",
      "  '5.765958567004418' '0.25749999999999995' '3.2233952149981633']\n",
      " ['MiddlePhalanxOutlineCorrect' '0.247' '0.352' '0.43833333333333335'\n",
      "  '19.338266527978703' '0.44499999999999995' '5.858533051999984']\n",
      " ['MiddlePhalanxTW' '0.439' '0.416' '0.3784461152882206'\n",
      "  '5.761900195007911' '0.40601503759398494' '3.2111663759860676']\n",
      " ['MoteStrain' '0.121' '0.165' '0.46006389776357826' '1.630086855002446'\n",
      "  '0.15095846645367417' '0.8643801739963237']\n",
      " ['NonInvasiveFatalECG_Thorax1' '0.171' '0.209' '0.6096692111959288'\n",
      "  '1234.4352623876184' '0.14147582697201022' '170.05694657564163']\n",
      " ['NonInvasiveFatalECG_Thorax2' '0.120' '0.135' '0.526208651399491'\n",
      "  '1265.492125541903' '0.12417302798982188' '174.72359622363']\n",
      " ['OSULeaf' '0.479' '0.409' '0.1074380165289256' '8.759781696018763'\n",
      "  '0.5413223140495868' '4.010478703974513']\n",
      " ['OliveOil' '0.133' '0.167' '0.7' '2.343024403002346' '0.7'\n",
      "  '1.1115353290224448']\n",
      " ['PhalangesOutlinesCorrect' '0.239' '0.272' '0.25757575757575757'\n",
      "  '210.09088478889316' '0.27622377622377625' '83.35346517525613']\n",
      " ['Phoneme' '0.891' '0.772' '0.7505274261603375' '14.16287470998941'\n",
      "  '0.9166666666666666' '5.0915008609881625']\n",
      " ['Plane' '0.038' '0.000' '0.0' '4.492473420017632' '0.01904761904761909'\n",
      "  '2.4936496160225943']\n",
      " ['ProximalPhalanxOutlineAgeGroup' '0.215' '0.195' '0.18536585365853664'\n",
      "  '47.43935503065586' '0.1658536585365854' '18.701147775165737']\n",
      " ['ProximalPhalanxOutlineCorrect' '0.192' '0.216' '0.13058419243986252'\n",
      "  '70.43491224385798' '0.18213058419243988' '27.73604399431497']\n",
      " ['ProximalPhalanxTW' '0.292' '0.263' '0.20750000000000002'\n",
      "  '7.710032109986059' '0.19499999999999995' '4.315486361010699']\n",
      " ['RefrigerationDevices' '0.605' '0.536' '0.45866666666666667'\n",
      "  '252.31475595291704' '0.6453333333333333' '33.54143838118762']\n",
      " ['ScreenType' '0.640' '0.603' '0.46399999999999997' '19.547373150999192'\n",
      "  '0.688' '7.914247716020327']\n",
      " ['ShapeletSim' '0.461' '0.350' '0.5' '1.7677283720113337'\n",
      "  '0.5055555555555555' '0.8653313250106294']\n",
      " ['ShapesAll' '0.248' '0.232' '0.4833333333333333' '307.54226871579885'\n",
      "  '0.43833333333333335' '40.05377131141722']\n",
      " ['SmallKitchenAppliances' '0.659' '0.357' '0.2773333333333333'\n",
      "  '246.52027956489474' '0.608' '33.37780389096588']\n",
      " ['SonyAIBORobotSurface' '0.305' '0.275' '0.5707154742096505'\n",
      "  '1.7487845940049738' '0.15973377703826952' '0.8696870600106195']\n",
      " ['SonyAIBORobotSurfaceII' '0.141' '0.169' '0.17208814270724027'\n",
      "  '1.763479665009072' '0.1636935991605456' '0.8783312749874312']\n",
      " ['StarLightCurves' '0.151' '0.093' '0.04188926663428849'\n",
      "  '1027.2817286895588' '0.07977173385138414' '127.89477120991796']\n",
      " ['Strawberry' '0.062' '0.060' '0.0538336052202284' '95.96922719012946'\n",
      "  '0.0538336052202284' '19.219221386127174']\n",
      " ['SwedishLeaf' '0.211' '0.208' '0.07040000000000002' '80.59862640313804'\n",
      "  '0.18079999999999996' '23.968330128118396']\n",
      " ['Symbols' '0.100' '0.050' '0.7748743718592965' '1.8167314929887652'\n",
      "  '0.24020100502512565' '0.8883297680004034']\n",
      " ['ToeSegmentation1' '0.320' '0.228' '0.4605263157894737'\n",
      "  '2.242265288019553' '0.38596491228070173' '1.1869833559903782']\n",
      " ['ToeSegmentation2' '0.192' '0.162' '0.2846153846153846'\n",
      "  '2.27275105699664' '0.29230769230769227' '1.170073889021296']\n",
      " ['Trace' '0.240' '0.000' '0.0' '4.233538462023716' '0.28'\n",
      "  '2.208416533016134']\n",
      " ['TwoLeadECG' '0.253' '0.096' '0.49956101843722567' '1.639842258009594'\n",
      "  '0.30465320456540823' '0.8846033570007421']\n",
      " ['Two_Patterns' '0.090' '0.000' '0.09150000000000003'\n",
      "  '34.32766931102378' '0.14775000000000005' '19.277311450015986']\n",
      " ['UWaveGestureLibraryAll' '0.052' '0.108' '0.3112786152987158'\n",
      "  '814.5173629475757' '0.05611390284757123' '104.67743703350425']\n",
      " ['Wine' '0.389' '0.426' '0.5' '2.744233081000857' '0.5'\n",
      "  '1.429480004007928']\n",
      " ['WordsSynonyms' '0.382' '0.351' '0.6269592476489028'\n",
      "  '10.496242002991494' '0.49216300940438873' '5.568121166987112']\n",
      " ['Worms' '0.635' '0.536' '0.49723756906077343' '68.61305598262697'\n",
      "  '0.6298342541436464' '10.164013951085508']\n",
      " ['WormsTwoClass' '0.414' '0.337' '0.4696132596685083' '68.5181339615956'\n",
      "  '0.4917127071823204' '10.191812179982662']\n",
      " ['synthetic_control' '0.120' '0.007' '0.17666666666666664'\n",
      "  '10.739913162979065' '0.2433333333333333' '6.017358184006298']\n",
      " ['uWaveGestureLibrary_X' '0.261' '0.273' '0.2805695142378559'\n",
      "  '292.1977254720405' '0.25013958682300386' '49.37008660286665']\n",
      " ['uWaveGestureLibrary_Y' '0.338' '0.366' '0.38190954773869346'\n",
      "  '292.30519205052406' '0.3174204355108877' '49.401677154935896']\n",
      " ['uWaveGestureLibrary_Z' '0.35' '0.342' '0.32244556113902845'\n",
      "  '292.2772204261273' '0.2970407593523171' '49.603060017339885']\n",
      " ['wafer' '0.005' '0.020' '0.004866969500324503' '185.03571289312094'\n",
      "  '0.007138221933809197' '47.95057901274413']\n",
      " ['yoga' '0.170' '0.164' '0.374' '13.262922360998346'\n",
      "  '0.20833333333333337' '6.097515064990148']]\n"
     ]
    }
   ],
   "source": [
    "with open(table_name, 'rb') as f:\n",
    "    perf_table_net = pickle.load(f)\n",
    "    \n",
    "print(perf_table_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_targ = ['InsectWingbeatSound', 'Phoneme']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of all data sets in UCR Archive\n",
    "PATH = 'UCR_TS_Archive_2015/'\n",
    "data_sets = []\n",
    "\n",
    "for folder_PATH in glob(PATH+'*/'):\n",
    "    \n",
    "    ds = folder_PATH.split(\"/\")[-2]\n",
    "    data_sets.append(ds)\n",
    "    \n",
    "data_sets = np.sort(data_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only augmented data sets\n",
    "ds_aug_list = []\n",
    "for folder_PATH in glob('Augmented_data_sets/'+'*'):\n",
    "    ds_aug = folder_PATH.split(\"/\")[-1]\n",
    "    ds = ds_aug.split(\"_\")[:-1]\n",
    "    ds_aug_list.append('_'.join(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'ResNet-DatAug_performance'\n",
    "continue_run = True\n",
    "continue_ds = True\n",
    "\n",
    "if continue_run:\n",
    "    with open(table_name, 'rb') as f:\n",
    "        perf_table_net = pickle.load(f)\n",
    "    ds_idx = np.where( data_sets == perf_table_net[-1,0] )[0][0] + 2\n",
    "else:\n",
    "    perf_table_net = np.array(['Data set', '1NN-ED', '1NN-DTW',\n",
    "                               'MLP', 'FCN', 'ResNet-DatAug', 'Run time ResNet-DatAug'])\n",
    "    ds_idx = 0\n",
    "    \n",
    "ds_done = []\n",
    "if continue_ds:\n",
    "    ds_done = perf_table_net[:,0]\n",
    "    ds_idx = 0\n",
    "    ds_done = np.concatenate((ds_done,np.array(['CinC_ECG_torso','HandOutlines']))) # Out Of Memory errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read previous performance results on UCR Archive\n",
    "UCR_results = {}\n",
    "\n",
    "lines = [line.rstrip('\\n') for line in open('UCR_results.txt')]\n",
    "\n",
    "for line in lines:\n",
    "    ds,nn_ed,nn_dtw,mlp,fcn,resnet,cote,shape_dtw = line.split(\",\")\n",
    "    UCR_results[ds] = ([nn_ed,nn_dtw,mlp,fcn,resnet,cote,shape_dtw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################# 50words #################################\n",
      "\n",
      "################################# Adiac #################################\n",
      "Length of augmented training set: 99390\n",
      "[[21.          1.598       1.5994     ...  1.5642      1.5709\n",
      "   1.5929    ]\n",
      " [27.          1.7011      1.6706     ...  1.5197      1.6025\n",
      "   1.6702    ]\n",
      " [20.          1.7223      1.6953     ...  1.6418      1.695\n",
      "   1.7085    ]\n",
      " ...\n",
      " [35.          1.4228      1.3134764  ...  1.68613135  1.62563769\n",
      "   1.5323    ]\n",
      " [13.          1.6397      1.63571585 ...  1.58464498  1.60932413\n",
      "   1.6209    ]\n",
      " [11.          1.6057      1.50163977 ...  1.70765681  1.70366438\n",
      "   1.6652    ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/tensorflow_p3/lib/python3.6/site-packages/tensorflow/python/client/session.py:1711: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 69573 samples, validate on 29817 samples\n",
      "Epoch 1/100\n",
      "69573/69573 [==============================] - 192s 3ms/step - loss: 2.2505 - acc: 0.4837 - val_loss: 3.7506 - val_acc: 0.0815\n",
      "Epoch 2/100\n",
      "69573/69573 [==============================] - 191s 3ms/step - loss: 1.1455 - acc: 0.7910 - val_loss: 2.7511 - val_acc: 0.2130\n",
      "Epoch 3/100\n",
      "59000/69573 [========================>.....] - ETA: 25s - loss: 0.7590 - acc: 0.8667"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-02b68d29f4f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m                     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                     verbose=1)\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow_p3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1000\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/tensorflow_p3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/tensorflow_p3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow_p3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow_p3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow_p3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow_p3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow_p3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow_p3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow_p3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for ds in data_sets[ds_idx:]:\n",
    "    \n",
    "    print('\\n################################# ' + ds + ' #################################')\n",
    "    \n",
    "    if ds in ds_done:\n",
    "        continue\n",
    "    \n",
    "    if ds not in ds_aug_list:\n",
    "        continue\n",
    "    \n",
    "    perf_table_line = np.array([ds, UCR_results[ds][0], UCR_results[ds][1], UCR_results[ds][2], UCR_results[ds][3]])\n",
    "    \n",
    "    \n",
    "    # Test set\n",
    "    with open(PATH + ds + str('/') + ds + '_TEST', 'r') as f:\n",
    "        \n",
    "        test = f.read().splitlines()\n",
    "        data_set_test = np.array([test[0].split(\",\")])\n",
    "        \n",
    "        for line in test[1:]:\n",
    "            data_set_test = np.append(data_set_test, [line.split(\",\")], axis=0)\n",
    "            \n",
    "    # Augmented training set\n",
    "    with open('Augmented_data_sets/' + ds + '_augmented', 'rb') as f:\n",
    "        augmented_data_set = pickle.load(f)\n",
    "    \n",
    "    # Remove NanNs\n",
    "    augmented_data_set = augmented_data_set[~np.isnan(augmented_data_set).any(axis=1)]\n",
    "\n",
    "    print('Length of augmented training set: ' + str(len(augmented_data_set)))\n",
    "    print(augmented_data_set)\n",
    "    \n",
    "\n",
    "    \n",
    "    # --------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "    # Set up training and test set\n",
    "    train_size_aug = len(augmented_data_set)\n",
    "    test_size = len(data_set_test)\n",
    "    ts_length = len(data_set_test[0])-1\n",
    "\n",
    "    X_train_aug = np.zeros((train_size_aug, ts_length))\n",
    "    y_train_aug = np.zeros(train_size_aug)\n",
    "\n",
    "    X_test = np.zeros((test_size, ts_length))\n",
    "    y_test = np.zeros(test_size)\n",
    "\n",
    "    for i in range(ts_length+1):\n",
    "        # Test\n",
    "        for j in range(test_size):\n",
    "            if i == 0:\n",
    "                y_test[j] = int(data_set_test[j][0])\n",
    "            else:\n",
    "                X_test[j][i-1] = float(data_set_test[j][i])\n",
    "        # Train\n",
    "        for j in range(train_size_aug):\n",
    "            if i == 0:\n",
    "                y_train_aug[j] = int(augmented_data_set[j][0])\n",
    "            else:\n",
    "                X_train_aug[j][i-1] = float(augmented_data_set[j][i])\n",
    "\n",
    "    # Make sure the labels are integers\n",
    "    y_test = y_test.astype(int)\n",
    "    y_train_aug = y_train_aug.astype(int)\n",
    "\n",
    "    # Make sure the labels are zero indexed\n",
    "    num_classes = len(np.unique(y_test))\n",
    "\n",
    "    idx = 0\n",
    "    for label in np.unique(y_test):\n",
    "        y_test[np.where( y_test == label )] = idx\n",
    "        idx += 1\n",
    "\n",
    "    # Convert labels to one-hot encoding\n",
    "    y_train_aug_onehot = np.zeros((train_size_aug, num_classes))\n",
    "    y_train_aug_onehot[np.arange(train_size_aug), y_train_aug] = 1\n",
    "\n",
    "\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    \n",
    "    # Reset tensorflow graph\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    # Setup model\n",
    "    model = ResNet_wang(ts_length, num_classes)\n",
    "    \n",
    "    # Set up data for Tensorflow model\n",
    "    X_train = np.reshape(X_train_aug,(X_train_aug.shape[0],X_train_aug.shape[1],1))\n",
    "    y_train_onehot = np.reshape(y_train_aug_onehot,(train_size_aug, num_classes))\n",
    "\n",
    "    # Optimizers\n",
    "    sgd = SGD(lr=0.001, decay=1e-6)\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.01)\n",
    "\n",
    "    # Train the model\n",
    "    batch_size = 1000\n",
    "    epochs = 100\n",
    "    validation_split = 0.3\n",
    "    \n",
    "    \n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train_onehot,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=validation_split,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)\n",
    "    \n",
    "\n",
    "    # Evaluate the model with test data\n",
    "    X_test = np.reshape(X_test,(X_test.shape[0],X_test.shape[1],1))\n",
    "    class_probs = model.predict(X_test)\n",
    "    y_pred = np.argmax(class_probs, axis=1)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    err_rate = error_rate(y_test, y_pred)\n",
    "    \n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "    print('\\nTime (with data augmentation): ' + str(elapsed))\n",
    "    print('Classification accuracy: ' + str(acc))\n",
    "    print('Error rate: ' + str(err_rate))\n",
    "    \n",
    "\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    \n",
    "    # Append and save table\n",
    "    perf_table_line = np.concatenate((perf_table_line,np.array([err_rate, elapsed])))\n",
    "    perf_table_net = np.row_stack((perf_table_net,perf_table_line))\n",
    "\n",
    "    with open(table_name, 'wb') as f:\n",
    "        pickle.dump(perf_table_net, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort and save performance table\n",
    "sorted_idx = np.argsort(perf_table_net[:,0])\n",
    "perf_table_net = perf_table_net[sorted_idx]\n",
    "\n",
    "with open(table_name, 'wb') as f:\n",
    "    pickle.dump(perf_table_net, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(table_name, 'rb') as f:\n",
    "    perf_table_net = pickle.load(f)\n",
    "    \n",
    "print(perf_table_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
