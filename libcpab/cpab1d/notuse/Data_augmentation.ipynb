{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is included to show what libraries are imported and used in the project\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "import itertools\n",
    "\n",
    "from glob import glob\n",
    "from scipy.linalg import expm\n",
    "import bisect\n",
    "from numpy import linalg as LA\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "from IPython.display import HTML\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions that perform a random transformation (Based on Freifeld article)\n",
    "\n",
    "# Generate L matrix from eq. 10\n",
    "def generate_L(N_p):\n",
    "    rows = N_p - 1\n",
    "    cols = 2 * N_p\n",
    "    \n",
    "    delta = float(1 / N_p)\n",
    "    \n",
    "    L = np.zeros((rows, cols))\n",
    "    \n",
    "    for i in range(rows):\n",
    "        L[i][2*i] = (i+1) * delta\n",
    "        L[i][2*i+1] = 1\n",
    "        L[i][2*i+2] = -(i+1) * delta\n",
    "        L[i][2*i+3] = -1\n",
    "    \n",
    "    return L\n",
    "\n",
    "\n",
    "# Find basis of null space of matrix via SVD\n",
    "def nullspace(A, atol=1e-16, rtol=0):\n",
    "    \"\"\"Compute an approximate basis for the nullspace of A.\n",
    "\n",
    "    The algorithm used by this function is based on the singular value\n",
    "    decomposition of `A`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : ndarray\n",
    "        A should be at most 2-D.  A 1-D array with length k will be treated\n",
    "        as a 2-D with shape (1, k)\n",
    "    atol : float\n",
    "        The absolute tolerance for a zero singular value.  Singular values\n",
    "        smaller than `atol` are considered to be zero.\n",
    "    rtol : float\n",
    "        The relative tolerance.  Singular values less than rtol*smax are\n",
    "        considered to be zero, where smax is the largest singular value.\n",
    "\n",
    "    If both `atol` and `rtol` are positive, the combined tolerance is the\n",
    "    maximum of the two; that is::\n",
    "        tol = max(atol, rtol * smax)\n",
    "    Singular values smaller than `tol` are considered to be zero.\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    ns : ndarray\n",
    "        If `A` is an array with shape (m, k), then `ns` will be an array\n",
    "        with shape (k, n), where n is the estimated dimension of the\n",
    "        nullspace of `A`.  The columns of `ns` are a basis for the\n",
    "        nullspace; each element in numpy.dot(A, ns) will be approximately\n",
    "        zero.\n",
    "    \"\"\"\n",
    "\n",
    "    A = np.atleast_2d(A)\n",
    "    u, s, vh = np.linalg.svd(A)\n",
    "    tol = max(atol, rtol * s[0])\n",
    "    nnz = (s >= tol).sum()\n",
    "    ns = vh[nnz:].conj().T\n",
    "    return ns\n",
    "\n",
    "# Psi computation, Eq. 20 in Freifeld\n",
    "def psi_computation(x, a, b, t):\n",
    "    if a == 0:\n",
    "        psi = x + t*b\n",
    "    else:\n",
    "        psi = math.exp(t*a)*x + (b*(math.exp(t*a)-1))/a\n",
    "    \n",
    "    return psi\n",
    "\n",
    "# Transformation v1! (Algorithm 1 from Freifeld)\n",
    "def transformation_v1(P, A, U, N_step, N_p, t=1):\n",
    "    N_pts = len(U)\n",
    "    delta_t = float(t) / N_step\n",
    "    \n",
    "    phi = np.zeros(N_pts)\n",
    "    \n",
    "    for i in range(N_pts):\n",
    "        phi[i] = U[i]\n",
    "        \n",
    "        for j in range(N_step):\n",
    "            c = bisect.bisect_left(P[1:], phi[i])\n",
    "            if c == N_p:\n",
    "                c = c-1\n",
    "            a = A[2*c]\n",
    "            b = A[2*c+1]\n",
    "\n",
    "            phi[i] = psi_computation(phi[i], a, b, delta_t)\n",
    "        \n",
    "    return phi\n",
    "\n",
    "# Perform data augmentation\n",
    "def generate_new_data(sigma, B, X_train, N_step, N_p):\n",
    "    [D,d] = B.shape\n",
    "    [train_size,ts_length] = X_train.shape\n",
    "    \n",
    "    # Sample new transformation from Gaussian distribution\n",
    "    theta_new = np.random.multivariate_normal(mean=np.zeros(d), cov=sigma)\n",
    "\n",
    "    # Compute A matrix for new transformation\n",
    "    A = np.matmul(B, theta_new)\n",
    "\n",
    "    # Sample a data point from uniform distribution\n",
    "    i = random.randint(0, train_size-1)\n",
    "    x_i = X_train[i]\n",
    "\n",
    "    # Transform time series\n",
    "    x = np.linspace(0,1,ts_length)\n",
    "    x_trans = transformation_v1(tess, A, x, N_step, N_p)\n",
    "\n",
    "    # Interpolate values to correct interval\n",
    "    x_trans_resc = (x_trans - np.amin(x_trans)) / (np.amax(x_trans) - np.amin(x_trans))\n",
    "    T_x_i = np.interp(x, x_trans_resc, x_i)\n",
    "    \n",
    "    return T_x_i\n",
    "\n",
    "\n",
    "def remove_outliers(data):\n",
    "    # Compute norm of every data point\n",
    "    data_norm = LA.norm(data, axis=1)\n",
    "    \n",
    "    # Find mean and standard deviation\n",
    "    sd = np.std(data_norm)\n",
    "    mean = np.mean(data_norm)\n",
    "    \n",
    "    # Find indices for data points to be removed\n",
    "    indices = []\n",
    "    for i in range(len(data)):\n",
    "        if (data_norm[i] < mean - 3*sd or mean + 3*sd < data_norm[i]):\n",
    "            indices.append(i)\n",
    "            \n",
    "    # Remove outliers\n",
    "    return np.delete(data, indices, axis=0)\n",
    "\n",
    "\n",
    "# ------------------------------- Functions for Tensorflow -------------------------------\n",
    "# Transformation v2! (TENSORFLOW IMPLEMENTATION)\n",
    "def transformation_v2(A, U, N_step, N_p, t=1):\n",
    "    delta_t = float(t) / N_step\n",
    "    \n",
    "    phi = U\n",
    "    \n",
    "    for j in range(N_step):\n",
    "        \n",
    "        # Find cell index\n",
    "        idx = tf.floor(N_p * phi)\n",
    "        idx = tf.clip_by_value(idx, clip_value_min=0, clip_value_max=N_p-1)\n",
    "        idx = tf.cast(idx, tf.int32)\n",
    "        \n",
    "        # Fetch values from A (vector field)\n",
    "        a = tf.reshape(tf.gather(A, 2*idx), [-1])\n",
    "        b = tf.reshape(tf.gather(A, 2*idx+1), [-1])\n",
    "        \n",
    "        # Perform psi computation\n",
    "        phi = tf.where(tf.equal(a, 0), psi_a_eq_zero(phi, a, b, delta_t), psi_a_noteq_zero(phi, a, b, delta_t))\n",
    "        \n",
    "    return phi\n",
    "\n",
    "def psi_a_eq_zero(x, a, b, t):\n",
    "    tb = tf.multiply(t,b)\n",
    "    psi = tf.add(x, tb)\n",
    "    return psi\n",
    "\n",
    "def psi_a_noteq_zero(x, a, b, t):\n",
    "    c1 = tf.exp(tf.multiply(t, a))\n",
    "    c2 = tf.truediv(tf.multiply(b, tf.subtract(c1, 1)), a)\n",
    "    psi = tf.add(tf.multiply(c1, x), c2)\n",
    "    return psi\n",
    "\n",
    "def tf_linear_interpolation(x, x_trans, y, ts_length):\n",
    "    \n",
    "    # POSSIBLY RESCALE VALUES IN X_TRANS TO RANGE [0,1] !!!!!!!!!!!!!\n",
    "    \n",
    "    # Find nearest smaller neighbor\n",
    "    dist = tf.subtract(tf.reshape(x_trans, [-1, 1]), x)\n",
    "    \n",
    "    # Find index of interval in tessellation\n",
    "    greater_than_zero = tf.greater_equal(dist, 0)\n",
    "    idx = (ts_length-1) - tf.reduce_sum(tf.cast(greater_than_zero, tf.float32), axis=0)\n",
    "    idx = tf.clip_by_value(idx, clip_value_min=0, clip_value_max=ts_length-2)\n",
    "    idx = tf.cast(idx, tf.int32)\n",
    "    \n",
    "    # Fetch values from x_trans and y\n",
    "    x0 = tf.gather(x_trans, idx)\n",
    "    x1 = tf.gather(x_trans, idx+1)\n",
    "    y0 = tf.gather(y, idx)\n",
    "    y1 = tf.gather(y, idx+1)\n",
    "    \n",
    "    # Perform linear interpolation on points in x\n",
    "    #frac = tf.truediv(tf.subtract(y1, y0), tf.subtract(x1, x0))\n",
    "    #x_diff = tf.subtract(x, x0)\n",
    "    #y_interp = tf.add(y0, tf.multiply(x_diff, frac))\n",
    "    \n",
    "    y_interp = y0 + (x-x0) * ((y1-y0)/(x1-x0))\n",
    "    \n",
    "    return y_interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'UCR_TS_Archive_2015/'\n",
    "data_sets = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_list = []\n",
    "for folder_PATH in glob('transformations/'+'*'):\n",
    "    ds_trans = folder_PATH.split(\"/\")[-1]\n",
    "    ds = ds_trans.split(\"_\")[:-1]\n",
    "    ds_list.append(\"_\".join(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder_PATH in glob('Augmented_data_sets/'+'*'):\n",
    "    ds_aug = folder_PATH.split(\"/\")[-1]\n",
    "    ds = ds_aug.split(\"_\")[:-1]\n",
    "    ds_list.remove('_'.join(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_list # 'DiatomSizeReduction','50words'\n",
    "\n",
    "# ds_list_skip_transformation = ['PhalangesOutlinesCorrect', 'ProximalPhalanxOutlineCorrect', 'FordA','ElectricDevices']\n",
    "# ds_list_early_stopped = ['wafer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "continue_run = False\n",
    "skip_transformations = False\n",
    "#ds_list = ['wafer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiatomSizeReduction\n",
      "50words\n"
     ]
    }
   ],
   "source": [
    "for folder_PATH in glob(PATH+'*/'):\n",
    "    \n",
    "    ds = folder_PATH.split(\"/\")[-2]\n",
    "    if folder_PATH.split(\"/\")[-2] not in ds_list:\n",
    "        continue\n",
    "    data_sets[ds] = {}\n",
    "    \n",
    "    print(ds)\n",
    "    \n",
    "    with open(folder_PATH + ds + '_TRAIN', 'r') as f:\n",
    "        \n",
    "        train = f.read().splitlines()\n",
    "        data_sets[ds]['TRAIN'] = np.array([train[0].split(\",\")])\n",
    "        \n",
    "        for line in train[1:]:\n",
    "            data_sets[ds]['TRAIN'] = np.append(data_sets[ds]['TRAIN'], [line.split(\",\")], axis=0)\n",
    "            \n",
    "    with open(folder_PATH + ds + '_TEST', 'r') as f:\n",
    "        \n",
    "        test = f.read().splitlines()\n",
    "        data_sets[ds]['TEST'] = np.array([test[0].split(\",\")])\n",
    "        \n",
    "        for line in test[1:]:\n",
    "            data_sets[ds]['TEST'] = np.append(data_sets[ds]['TEST'], [line.split(\",\")], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################DiatomSizeReduction#################################\n",
      "\n",
      "Saving 16 data points\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/tensorflow_p3/lib/python3.6/site-packages/sklearn/decomposition/pca.py:423: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ = (S ** 2) / (n_samples - 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "array must not contain infs or NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-765870c8f63c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;31m# Generate new data point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_new_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigmas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_classes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0mdata_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-2afd6e11b936>\u001b[0m in \u001b[0;36mgenerate_new_data\u001b[0;34m(sigma, B, X_train, N_step, N_p)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;31m# Sample new transformation from Gaussian distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mtheta_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultivariate_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;31m# Compute A matrix for new transformation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.multivariate_normal\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/tensorflow_p3/lib/python3.6/site-packages/scipy/linalg/decomp_svd.py\u001b[0m in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \"\"\"\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_validated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'expected matrix'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow_p3/lib/python3.6/site-packages/scipy/_lib/_util.py\u001b[0m in \u001b[0;36m_asarray_validated\u001b[0;34m(a, check_finite, sparse_ok, objects_ok, mask_ok, as_inexact)\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'masked arrays are not supported'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0mtoarray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray_chkfinite\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcheck_finite\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobjects_ok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'O'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow_p3/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36masarray_chkfinite\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m   1231\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtypecodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AllFloat'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m         raise ValueError(\n\u001b[0;32m-> 1233\u001b[0;31m             \"array must not contain infs or NaNs\")\n\u001b[0m\u001b[1;32m   1234\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: array must not contain infs or NaNs"
     ]
    }
   ],
   "source": [
    "for ds in ds_list:\n",
    "    \n",
    "    print('#################################' + ds + '#################################')\n",
    "    \n",
    "    # --------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Set up training and test set\n",
    "    train_size = len(data_sets[ds]['TRAIN'])\n",
    "    test_size = len(data_sets[ds]['TEST'])\n",
    "    ts_length = len(data_sets[ds]['TRAIN'][0])-1\n",
    "\n",
    "    X_train = np.zeros((train_size, ts_length))\n",
    "    y_train = np.zeros(train_size)\n",
    "\n",
    "    X_test = np.zeros((test_size, ts_length))\n",
    "    y_test = np.zeros(test_size)\n",
    "\n",
    "    for i in range(ts_length+1):\n",
    "        # Train\n",
    "        for j in range(train_size):\n",
    "            if i == 0:\n",
    "                y_train[j] = int(data_sets[ds]['TRAIN'][j][0])\n",
    "            else:\n",
    "                X_train[j][i-1] = float(data_sets[ds]['TRAIN'][j][i])\n",
    "        # Test\n",
    "        for j in range(test_size):\n",
    "            if i == 0:\n",
    "                y_test[j] = int(data_sets[ds]['TEST'][j][0])\n",
    "            else:\n",
    "                X_test[j][i-1] = float(data_sets[ds]['TEST'][j][i])\n",
    "\n",
    "    if not np.all(y_train):\n",
    "        zero_idx = True\n",
    "    else:\n",
    "        zero_idx = False\n",
    "\n",
    "    # Make sure the labels are integers\n",
    "    y_train = y_train.astype(int)\n",
    "    y_test = y_test.astype(int)\n",
    "\n",
    "    # Make sure the labels are zero indexed\n",
    "    num_classes = len(np.unique(y_train))\n",
    "\n",
    "    idx = 0\n",
    "    for label in np.unique(y_train):\n",
    "        y_train[np.where( y_train == label )] = idx\n",
    "        y_test[np.where( y_test == label )] = idx\n",
    "        idx += 1\n",
    "\n",
    "    # Convert labels to one-hot encoding\n",
    "    y_train_onehot = np.zeros((train_size, num_classes))\n",
    "    y_train_onehot[np.arange(train_size), y_train] = 1\n",
    "\n",
    "\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "    # Get indices for different classes\n",
    "    class_indices = {}\n",
    "\n",
    "    for label in range(num_classes):\n",
    "        class_indices[label] = np.where( y_train == label )[0]\n",
    "\n",
    "        \n",
    "    # Load transformations\n",
    "    with open('transformations/' + ds + '_transformations', 'rb') as f:\n",
    "        ds_transformations_pkl = pickle.load(f)\n",
    "        \n",
    "\n",
    "    # Seperate transformations into classes\n",
    "    labels = []\n",
    "    \n",
    "    for label in range(num_classes):\n",
    "        labels = labels + [label] * len(class_indices[label])**2\n",
    "    \n",
    "    # Remove pairs to reduce computations\n",
    "    while (len(labels) > 150000 and skip_transformations):\n",
    "        del labels[::4]\n",
    "       \n",
    "    \n",
    "    split_indices = []\n",
    "    for label in range(num_classes):\n",
    "        split_indices.append(np.count_nonzero(np.array(labels) == label))\n",
    "   \n",
    "    split_indices = np.cumsum(split_indices)[:-1]\n",
    "    \n",
    "    theta_classes = np.split(ds_transformations_pkl, split_indices)\n",
    "\n",
    "    \n",
    "    # Clean set of transformations\n",
    "\n",
    "    # Remove NaNs\n",
    "    for i in range(num_classes):\n",
    "        theta_classes[i] = theta_classes[i][~np.isnan(theta_classes[i]).any(axis=1)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Number of intervals and number of cell intersections\n",
    "    N_p = 10\n",
    "    N_v = N_p + 1\n",
    "\n",
    "    N_step = 100\n",
    "\n",
    "    # Generate tesselation in 1D\n",
    "    tess = np.linspace(0,1,N_v)\n",
    "\n",
    "    # Generate L\n",
    "    L = generate_L(N_p)\n",
    "\n",
    "    # Find basis for null(L)\n",
    "    B = nullspace(L)\n",
    "    [D,d] = B.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Extract data points and perform PCA for all classes\n",
    "    X_train_classes = {}\n",
    "    class_sizes = []\n",
    "    sigmas = {}\n",
    "\n",
    "    for idx in range(num_classes):\n",
    "        X_train_classes[idx] = X_train[np.where( y_train == idx )]\n",
    "        class_sizes.append(len(X_train_classes[idx]))\n",
    "\n",
    "        pca = PCA(n_components=None)\n",
    "        pca.fit(remove_outliers(theta_classes[idx]))\n",
    "\n",
    "        sigmas[idx] = pca.get_covariance()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Perform data augmentation on data sets\n",
    "\n",
    "    # Set up numpy array to contain augmented data set\n",
    "    if continue_run:\n",
    "        with open('Augmented_data_sets/' + ds + '_augmented', 'rb') as f:\n",
    "            augmented_data_set = pickle.load(f)\n",
    "        idx = len(augmented_data_set)\n",
    "        continue_run = False\n",
    "    else:\n",
    "        augmented_data_set = np.zeros((train_size, ts_length+1))\n",
    "        augmented_data_set[:,1:] = X_train\n",
    "        augmented_data_set[:,0] = y_train\n",
    "        idx = 0\n",
    "\n",
    "    # Run loop until stopped manually\n",
    "    while idx < 100000:\n",
    "        # Save transformation to file\n",
    "        if (idx % 1000 == 0):\n",
    "            with open('Augmented_data_sets/' + ds + '_augmented', 'wb') as f:\n",
    "                pickle.dump(augmented_data_set, f)\n",
    "            print('\\nSaving ' + str(len(augmented_data_set)) + ' data points\\n')\n",
    "\n",
    "        class_idx = random.randint(0, num_classes-1)\n",
    "\n",
    "        # Generate new data point\n",
    "        new_data = generate_new_data(sigmas[class_idx], B, X_train_classes[class_idx], N_step, N_p)\n",
    "        data_point = np.insert(new_data, 0, class_idx)\n",
    "\n",
    "        # Append to augmented data set\n",
    "        augmented_data_set = np.row_stack((augmented_data_set,data_point))\n",
    "\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
